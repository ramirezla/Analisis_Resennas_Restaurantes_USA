{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción Transformación y Carga (ETL)\n",
    "\n",
    "### Fuente de datos:\n",
    "+ [DataSet de Google Maps !!!](https://drive.google.com/drive/folders/1Wf7YkxA0aHI3GpoHc9Nh8_scf5BbD4DA?usp=drive_link):  Google Drive Henry.\n",
    "\n",
    "+ [DataSet de Yelp !!!](https://drive.google.com/drive/folders/1TI-SsMnZsNP6t930olEEWbBQdo_yuIZF?usp=drive_link):  Google Drive Henry..\n",
    "<br/>\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import os                       # Libreria para manejo de arhivos y directorios del sistema\n",
    "import glob                     # crear listas de archivos a partir de búsquedas con comodines en un directorio\n",
    "import pyarrow as pa\n",
    "import jsonlines                # Permite procesar un registro por vez\n",
    "import pyarrow.parquet as pq    # Permite leer archivos en formato parquet\n",
    "import pickle5 as pickle        # Permite leer archivos con formatp pkl, para archivos version 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Funciones generales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que permite buscar archivos en un directorio segun un filtro deseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe el directorio a trabajar y el patron o filtro que se desea mostrar.\n",
    "\n",
    "def listar_archivos_segun_patron_con_directorio(directorio, patron):\n",
    "    return glob.glob(directorio + patron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "+ DataSet de Yelp !!!\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Datasets: checkin`\n",
    "\n",
    "Tipo de archivo JSon\n",
    "\n",
    "Procedimieto que permite separar el dataset checkin en 1 archivos tipo JSon.<br>\n",
    "El datasets checkin tiene aproximadamente: 131.930 registros, por tamaño del archivo debe ser separado para que pueda ser subido al repositorio.<br>\n",
    "131930 // 2 aproximadamente 65965<br>\n",
    "Se abre el archivo JSon y se salva tipo parquet con la misma cantidad de registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_checkin_json = \"../Datasets/Datasets_Yelp/checkin/checkin.json\"\n",
    "cantidad_registros = 65965                                                      # Cantidad de registros por archivos\n",
    "with jsonlines.open(archivo_checkin_json) as reader:\n",
    "    lista_datos = list(reader)                                                  # Lee todos los registros y crea una lista\n",
    "    total_registros = len(lista_datos)                                          # Se tiene el total de registros, esto para separar en varios archivos.\n",
    "    for ind in range(0, total_registros, cantidad_registros):\n",
    "        iformacion_registros = lista_datos[ind:ind+cantidad_registros]          # Se extrae grupos de cantidad_registros registros\n",
    "        df = pd.DataFrame(iformacion_registros)                                 # Se crea un dataframe con el grupo de registros extraidos\n",
    "        table = pa.Table.from_pandas(df)                                        # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "        nombre_archivo = \"checkin_\" + str(ind//cantidad_registros) + \".parquet\" # Se arma el nombre del archivo\n",
    "        ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/checkin/', nombre_archivo)    # La ruta donde se guardara el archivo\n",
    "        pq.write_table(table, ruta_archivo)                                     # Se escribe en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/checkin/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"checkin_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_checkin_concatenado = pd.concat(df_list)     # Se concatenan todos los DataFrames en uno solo\n",
    "df_checkin_concatenado = df_checkin_concatenado.reset_index(drop=True)    # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--0iUa4sNDFiZFrAdIWhZQ</td>\n",
       "      <td>2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--30_8IhuyMHbSOcNWd6DQ</td>\n",
       "      <td>2013-06-14 23:29:17, 2014-08-13 23:20:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--7jw19RH9JKXgFohspgQw</td>\n",
       "      <td>2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               date\n",
       "0  ---kPU91CF4Lq2-WlRu9Lw  2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
       "1  --0iUa4sNDFiZFrAdIWhZQ  2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
       "2  --30_8IhuyMHbSOcNWd6DQ           2013-06-14 23:29:17, 2014-08-13 23:20:22\n",
       "3  --7PUidqRWpRSpXebiyxTg  2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
       "4  --7jw19RH9JKXgFohspgQw  2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checkin_concatenado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131930, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checkin_concatenado.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario del dataset checkin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Column | Dtype | RealType |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| 0 | business_id | object | string |\n",
    "| 3 | date | float64 | list date YYYY-MM-DD |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su manipulacion se reduce el dataset a 5 años de registros y se ajustan los campos:<br> \n",
    ". date al formato: YYYY-MM-DD<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales desde:  2009-12-30 02:53:27, 2010-02-11 18:43:45, 2010-04-25 03:57:16, 2010-08-07 01:59:21, 2010-09-27 23:11:18, 2010-10-01 23:16:35, 2010-10-14 03:05:41, 2010-10-15 14:41:50, 2010-10-16 13:02:26, 2010-10-23 23:30:18, 2010-11-14 16:44:16, 2010-11-26 18:14:29, 2010-11-27 16:12:46, 2010-12-01 12:30:33, 2010-12-20 21:11:15, 2011-01-21 23:19:57, 2011-02-02 16:30:10, 2011-02-05 03:00:15, 2011-02-05 03:52:29, 2011-03-13 17:52:54, 2011-03-15 22:22:34, 2011-04-08 16:21:27, 2011-04-09 17:04:31, 2011-05-14 17:17:47, 2011-05-14 17:18:35, 2011-05-14 17:18:37, 2011-05-15 03:02:25, 2011-06-05 22:58:18, 2011-06-14 23:52:02, 2011-07-24 17:03:19, 2011-07-28 15:25:57, 2011-07-30 14:33:13, 2011-07-30 14:42:04, 2011-08-06 05:16:21, 2011-08-18 05:05:33, 2011-08-29 00:40:44, 2011-10-13 20:40:52, 2011-10-29 10:41:31, 2011-11-02 16:11:44, 2011-11-02 16:11:58, 2011-11-03 16:30:57, 2011-11-06 05:11:24, 2011-11-06 15:02:36, 2011-11-07 18:33:14, 2011-11-12 19:46:30, 2011-11-13 14:25:42, 2011-11-13 14:28:10, 2011-11-16 23:50:05, 2011-11-20 04:39:42, 2011-11-25 15:32:12, 2011-11-27 04:43:00, 2011-12-02 05:51:13, 2011-12-03 18:38:07, 2011-12-06 17:25:40, 2011-12-08 04:47:39, 2011-12-28 00:04:08, 2012-01-07 15:25:22, 2012-01-07 15:28:55, 2012-01-07 18:48:24, 2012-01-16 00:58:41, 2012-01-21 23:58:54, 2012-01-29 20:38:04, 2012-01-29 23:53:56, 2012-02-03 03:39:13, 2012-02-04 08:17:04, 2012-02-04 23:05:52, 2012-02-28 05:20:38, 2012-03-03 17:30:34, 2012-03-04 18:24:21, 2012-03-09 06:25:00, 2012-03-10 02:40:49, 2012-03-10 12:26:26, 2012-03-10 21:56:13, 2012-03-12 15:54:35, 2012-03-15 12:38:43, 2012-03-24 04:33:02, 2012-04-01 15:28:24, 2012-04-08 03:38:03, 2012-04-13 16:30:24, 2012-04-14 01:02:39, 2012-04-14 14:48:33, 2012-04-14 22:03:58, 2012-04-15 10:08:20, 2012-05-05 14:10:16, 2012-05-06 10:34:54, 2012-05-12 22:08:03, 2012-05-18 23:58:07, 2012-05-19 14:48:38, 2012-05-20 17:04:09, 2012-05-25 11:45:39, 2012-05-28 11:13:08, 2012-05-28 11:37:49, 2012-06-04 03:00:56, 2012-06-09 12:25:30, 2012-06-09 15:22:41, 2012-06-09 22:36:25, 2012-06-09 23:36:46, 2012-06-09 23:40:08, 2012-06-14 22:42:00, 2012-06-21 15:43:15, 2012-06-24 00:26:50, 2012-06-29 05:09:07, 2012-07-05 10:45:27, 2012-07-07 13:03:10, 2012-07-08 21:22:13, 2012-07-11 11:14:20, 2012-07-13 17:17:39, 2012-07-16 10:53:43, 2012-08-01 15:58:02, 2012-08-05 10:32:05, 2012-08-11 13:19:53, 2012-08-12 16:07:08, 2012-08-21 01:17:33, 2012-08-26 16:09:58, 2012-08-27 00:46:43, 2012-08-27 00:53:28, 2012-08-27 00:56:04, 2012-08-31 15:53:13, 2012-09-03 10:44:06, 2012-09-12 02:14:12, 2012-09-13 16:12:44, 2012-09-13 16:13:33, 2012-09-19 19:10:00, 2012-09-23 16:43:23, 2012-09-26 00:07:45, 2012-09-28 08:44:12, 2012-10-06 14:18:34, 2012-10-22 21:53:47, 2012-10-28 19:40:24, 2012-10-29 10:29:53, 2012-11-03 04:09:46, 2012-11-03 04:10:45, 2012-11-03 13:55:28, 2012-11-11 02:25:50, 2012-11-11 11:40:32, 2012-11-18 11:58:50, 2012-11-22 11:20:37, 2012-11-30 14:31:51, 2012-12-01 10:38:13, 2012-12-02 05:01:40, 2012-12-06 06:51:42, 2012-12-08 16:21:32, 2012-12-08 16:29:38, 2012-12-09 19:35:27, 2012-12-15 12:08:22, 2012-12-16 21:30:50, 2012-12-17 11:55:01, 2012-12-19 12:10:16, 2012-12-21 12:26:34, 2012-12-27 17:50:37, 2012-12-28 03:28:37, 2012-12-28 04:54:36, 2012-12-31 16:00:45, 2013-01-09 00:01:07, 2013-01-12 12:04:11, 2013-01-13 11:32:53, 2013-01-19 18:35:35, 2013-01-20 12:56:20, 2013-01-24 10:35:35, 2013-01-28 21:57:43, 2013-02-03 17:06:23, 2013-02-05 10:42:10, 2013-02-09 11:36:36, 2013-02-09 22:44:25, 2013-02-10 15:56:17, 2013-02-13 00:04:05, 2013-02-16 18:40:18, 2013-02-17 11:27:42, 2013-02-19 15:22:04, 2013-02-22 10:53:21, 2013-02-24 01:41:43, 2013-02-24 02:02:35, 2013-02-26 12:18:50, 2013-03-03 00:13:35, 2013-03-03 16:09:01, 2013-03-05 13:16:41, 2013-03-10 18:31:34, 2013-03-17 07:19:44, 2013-03-17 11:16:57, 2013-03-22 10:23:23, 2013-03-23 16:21:09, 2013-03-23 17:28:29, 2013-03-26 13:24:17, 2013-03-26 17:27:39, 2013-03-30 14:55:48, 2013-03-31 18:13:29, 2013-04-07 10:43:25, 2013-04-09 00:11:06, 2013-04-10 12:29:14, 2013-04-25 03:31:51, 2013-05-05 05:21:22, 2013-05-08 10:13:55, 2013-05-12 14:27:30, 2013-05-14 09:40:42, 2013-05-24 09:50:15, 2013-05-26 05:03:33, 2013-05-26 05:10:36, 2013-05-26 11:26:34, 2013-05-27 10:41:03, 2013-05-28 01:14:46, 2013-05-29 13:03:00, 2013-06-02 17:21:59, 2013-06-02 17:23:41, 2013-06-04 09:40:37, 2013-06-06 09:35:04, 2013-06-11 10:11:10, 2013-06-24 23:04:48, 2013-06-25 09:59:38, 2013-07-05 13:32:50, 2013-07-07 16:19:57, 2013-07-08 22:31:44, 2013-07-09 12:29:57, 2013-07-14 01:59:32, 2013-07-15 13:24:57, 2013-07-20 13:37:27, 2013-07-20 13:51:44, 2013-07-21 10:18:22, 2013-07-25 03:15:49, 2013-08-07 13:06:55, 2013-08-08 23:34:36, 2013-08-10 13:46:38, 2013-08-11 00:02:10, 2013-08-17 18:47:25, 2013-08-20 18:09:48, 2013-08-21 16:40:04, 2013-08-24 07:07:04, 2013-08-25 17:26:34, 2013-08-26 04:22:05, 2013-08-31 04:12:01, 2013-08-31 16:43:25, 2013-09-04 16:33:48, 2013-09-04 21:50:26, 2013-09-05 13:42:14, 2013-09-13 00:34:05, 2013-09-14 14:17:07, 2013-09-19 12:37:44, 2013-09-23 03:58:46, 2013-09-27 01:25:41, 2013-09-28 14:32:00, 2013-10-02 23:59:32, 2013-10-06 02:26:28, 2013-10-11 11:39:19, 2013-10-19 23:05:25, 2013-10-19 23:05:27, 2013-10-19 23:06:59, 2013-10-25 17:06:33, 2013-10-27 03:50:28, 2013-10-31 03:50:22, 2013-11-09 00:30:46, 2013-11-09 16:17:59, 2013-11-09 17:15:27, 2013-11-15 23:35:30, 2013-11-16 05:39:19, 2013-11-16 14:51:48, 2013-11-17 15:51:24, 2013-11-18 01:16:09, 2013-11-24 18:13:16, 2013-11-28 16:14:39, 2013-12-06 21:38:57, 2013-12-13 04:38:42, 2013-12-14 00:05:13, 2013-12-14 03:45:51, 2013-12-17 17:25:28, 2013-12-19 02:05:44, 2013-12-22 17:19:41, 2013-12-26 17:59:13, 2013-12-27 02:47:39, 2013-12-28 03:23:33, 2014-01-01 16:45:24, 2014-01-03 21:56:05, 2014-01-04 16:26:21, 2014-01-05 14:01:19, 2014-01-12 02:35:26, 2014-01-16 17:13:18, 2014-01-26 15:37:27, 2014-01-27 15:54:16, 2014-01-31 01:05:25, 2014-02-07 19:54:03, 2014-02-12 02:54:47, 2014-02-15 05:16:09, 2014-02-15 17:14:43, 2014-02-22 18:26:44, 2014-02-26 17:41:21, 2014-03-04 16:53:49, 2014-03-14 03:53:18, 2014-03-18 01:47:40, 2014-03-21 16:22:34, 2014-03-27 22:14:15, 2014-03-29 20:36:07, 2014-03-30 13:17:03, 2014-04-13 14:03:17, 2014-04-18 02:55:04, 2014-04-20 19:49:54, 2014-04-22 11:40:27, 2014-04-29 13:50:13, 2014-05-03 18:30:09, 2014-05-10 02:42:03, 2014-05-18 15:41:08, 2014-05-20 19:08:05, 2014-05-22 20:01:06, 2014-05-28 19:31:42, 2014-05-31 16:07:48, 2014-06-02 12:54:29, 2014-06-03 19:25:08, 2014-06-05 13:13:15, 2014-06-07 14:48:14, 2014-06-08 15:21:54, 2014-06-09 12:51:44, 2014-06-18 15:59:22, 2014-06-20 03:15:48, 2014-06-22 00:58:43, 2014-06-23 12:52:19, 2014-06-24 13:02:15, 2014-06-29 13:39:40, 2014-06-30 16:06:54, 2014-07-08 14:00:36, 2014-07-13 14:30:43, 2014-07-14 13:02:11, 2014-07-17 13:18:03, 2014-07-22 12:49:23, 2014-08-02 03:28:47, 2014-08-03 14:55:45, 2014-08-04 13:00:05, 2014-08-04 16:53:11, 2014-08-05 07:04:48, 2014-08-11 12:54:23, 2014-08-16 01:48:37, 2014-08-25 11:34:26, 2014-08-30 21:19:57, 2014-09-14 14:30:30, 2014-09-20 18:58:55, 2014-09-22 11:34:31, 2014-09-27 22:04:14, 2014-09-27 22:08:50, 2014-09-27 22:50:52, 2014-10-04 13:31:09, 2014-10-05 22:09:44, 2014-10-05 22:10:05, 2014-10-06 13:39:14, 2014-10-11 15:14:20, 2014-10-13 14:43:53, 2014-10-18 04:17:10, 2014-10-19 15:06:10, 2014-10-26 13:18:55, 2014-10-26 14:13:17, 2014-10-26 22:01:29, 2014-10-26 22:02:36, 2014-11-01 15:16:57, 2014-11-02 17:19:10, 2014-11-02 17:20:30, 2014-11-05 14:33:11, 2014-11-08 13:51:34, 2014-11-09 22:30:32, 2014-11-09 22:30:33, 2014-11-16 05:50:23, 2014-11-16 19:27:55, 2014-11-16 19:28:05, 2014-11-16 19:57:00, 2014-11-17 12:37:41, 2014-11-22 23:20:12, 2014-11-22 23:21:45, 2014-11-23 17:37:45, 2014-11-28 18:56:58, 2014-11-29 21:59:37, 2014-12-01 00:15:38, 2014-12-05 19:47:28, 2014-12-06 20:51:47, 2014-12-07 21:05:13, 2014-12-08 04:53:03, 2014-12-13 20:42:48, 2014-12-13 20:50:52, 2014-12-20 16:53:00, 2014-12-21 21:17:44, 2014-12-21 21:19:14, 2014-12-21 23:20:23, 2014-12-23 17:35:12, 2015-01-06 01:21:04, 2015-01-11 04:01:22, 2015-01-23 02:40:07, 2015-01-24 14:24:14, 2015-01-24 22:41:31, 2015-01-26 12:36:30, 2015-02-14 03:26:14, 2015-02-16 00:05:28, 2015-02-16 04:53:30, 2015-02-19 14:33:35, 2015-02-21 02:42:22, 2015-02-21 17:54:20, 2015-02-21 17:57:32, 2015-02-21 18:30:41, 2015-02-22 16:44:00, 2015-02-26 17:35:38, 2015-02-27 02:39:38, 2015-03-05 01:49:19, 2015-03-08 18:27:03, 2015-03-08 22:07:31, 2015-03-08 22:09:05, 2015-03-10 12:28:09, 2015-03-14 02:19:44, 2015-03-15 15:35:12, 2015-03-22 01:52:54, 2015-03-22 05:10:36, 2015-03-24 01:13:26, 2015-03-26 17:10:12, 2015-03-28 21:00:05, 2015-03-31 11:57:22, 2015-04-01 23:17:23, 2015-04-03 12:52:31, 2015-04-11 23:48:56, 2015-04-11 23:52:17, 2015-04-12 20:32:00, 2015-04-12 20:33:05, 2015-04-22 23:50:01, 2015-05-02 04:06:05, 2015-05-16 15:35:15, 2015-05-25 16:26:44, 2015-05-25 16:27:03, 2015-05-26 17:24:05, 2015-05-26 19:28:52, 2015-05-30 14:03:18, 2015-06-07 23:28:03, 2015-06-14 14:50:31, 2015-06-27 07:38:27, 2015-06-28 15:44:20, 2015-07-01 22:21:42, 2015-07-07 13:28:09, 2015-07-12 20:01:22, 2015-07-25 21:58:26, 2015-07-26 22:40:24, 2015-08-08 16:40:12, 2015-08-08 21:56:18, 2015-08-19 22:08:17, 2015-08-23 22:17:35, 2015-08-23 22:18:13, 2015-09-06 13:43:34, 2015-09-12 17:28:54, 2015-09-12 23:43:28, 2015-09-13 00:34:02, 2015-09-27 14:03:19, 2015-09-27 16:42:07, 2015-09-27 16:43:06, 2015-10-03 00:16:17, 2015-10-03 18:04:00, 2015-10-04 18:55:05, 2015-10-04 18:59:18, 2015-10-18 03:29:35, 2015-10-25 18:42:54, 2015-10-25 18:43:06, 2015-10-29 00:14:53, 2015-10-29 22:08:50, 2015-10-31 15:05:04, 2015-11-01 17:46:27, 2015-11-01 17:46:40, 2015-11-08 14:47:11, 2015-11-08 14:47:35, 2015-11-10 14:17:06, 2015-11-22 04:28:22, 2015-11-22 15:56:15, 2015-11-22 16:33:58, 2015-12-13 17:03:11, 2015-12-14 02:00:54, 2015-12-15 14:18:09, 2015-12-16 13:24:59, 2015-12-20 19:59:21, 2015-12-20 19:59:40, 2016-01-02 19:00:36, 2016-01-09 21:20:27, 2016-01-15 17:35:36, 2016-01-18 17:50:18, 2016-01-18 17:51:06, 2016-01-18 18:21:38, 2016-01-24 17:16:38, 2016-02-10 19:28:16, 2016-02-14 21:10:25, 2016-02-14 21:10:58, 2016-02-21 00:55:52, 2016-02-21 18:19:49, 2016-02-21 18:20:35, 2016-02-27 05:17:34, 2016-02-27 15:04:05, 2016-02-27 15:14:58, 2016-02-27 23:17:03, 2016-02-28 00:56:55, 2016-03-04 13:31:15, 2016-03-21 05:01:31, 2016-03-27 21:54:41, 2016-04-13 16:45:49, 2016-04-17 16:02:37, 2016-04-23 21:53:08, 2016-04-30 18:36:23, 2016-05-07 14:37:41, 2016-05-08 17:37:13, 2016-05-16 17:49:33, 2016-05-18 15:49:05, 2016-05-22 14:45:39, 2016-05-27 12:28:41, 2016-05-30 17:42:26, 2016-06-23 00:11:43, 2016-06-26 12:53:22, 2016-06-26 19:41:02, 2016-07-03 15:26:32, 2016-07-09 16:43:07, 2016-07-21 16:07:36, 2016-07-24 16:05:50, 2016-08-07 18:34:29, 2016-08-18 11:58:11, 2016-08-20 17:42:56, 2016-08-28 11:17:37, 2016-09-25 17:46:16, 2016-09-30 16:39:02, 2016-10-13 16:15:42, 2016-10-22 23:45:08, 2016-10-27 14:11:20, 2016-10-28 18:25:45, 2016-11-07 16:39:00, 2016-11-09 17:42:14, 2016-11-13 17:18:27, 2016-11-20 16:43:12, 2016-12-01 04:12:37, 2016-12-02 01:30:09, 2016-12-06 17:09:01, 2016-12-15 15:32:33, 2016-12-29 03:15:18, 2017-01-07 16:11:48, 2017-01-20 17:10:07, 2017-01-22 04:19:20, 2017-01-24 08:35:11, 2017-02-02 22:39:24, 2017-02-05 16:52:19, 2017-02-12 15:47:34, 2017-02-13 00:30:01, 2017-02-16 16:47:20, 2017-02-18 16:07:30, 2017-03-02 01:33:37, 2017-03-05 02:32:14, 2017-03-18 15:22:35, 2017-03-24 16:09:07, 2017-03-25 18:17:09, 2017-04-06 01:25:55, 2017-04-06 16:07:40, 2017-05-14 14:47:12, 2017-05-14 20:21:08, 2017-05-31 23:29:58, 2017-06-01 00:07:16, 2017-06-11 17:01:10, 2017-06-12 21:23:27, 2017-06-14 20:57:20, 2017-06-17 13:12:20, 2017-06-18 16:09:49, 2017-06-23 22:22:14, 2017-07-06 18:02:18, 2017-07-08 20:15:28, 2017-07-13 22:15:59, 2017-07-14 16:48:52, 2017-07-16 17:19:55, 2017-07-16 17:57:22, 2017-07-23 16:05:45, 2017-08-02 12:23:33, 2017-08-19 15:47:05, 2017-09-01 19:17:59, 2017-09-03 04:07:26, 2017-09-03 15:04:40, 2017-10-07 21:23:53, 2017-10-20 23:36:37, 2017-10-22 02:53:17, 2017-10-22 13:56:01, 2017-10-24 15:58:57, 2017-10-29 16:03:27, 2017-11-07 17:38:05, 2017-11-23 00:39:03, 2017-11-25 03:40:34, 2017-11-25 17:50:05, 2017-12-03 21:55:33, 2017-12-16 08:43:14, 2017-12-24 15:08:38, 2017-12-27 17:53:56, 2018-01-07 21:30:38, 2018-01-12 00:14:57, 2018-01-23 00:49:33, 2018-01-28 13:33:37, 2018-01-30 15:37:35, 2018-01-30 17:34:58, 2018-03-03 22:16:06, 2018-03-10 16:47:37, 2018-03-11 00:15:30, 2018-03-21 14:23:17, 2018-03-24 14:35:25, 2018-04-07 14:35:46, 2018-04-08 00:08:36, 2018-04-12 15:49:42, 2018-04-13 16:15:05, 2018-04-19 21:43:34, 2018-05-05 21:57:16, 2018-05-10 16:25:48, 2018-05-16 22:39:38, 2018-05-24 02:00:09, 2018-05-25 15:00:15, 2018-06-06 16:43:22, 2018-06-09 23:26:22, 2018-06-10 15:31:10, 2018-06-10 16:19:10, 2018-06-24 22:12:00, 2018-06-24 22:12:16, 2018-07-02 02:15:09, 2018-07-20 14:04:48, 2018-08-04 23:13:32, 2018-08-09 22:40:38, 2018-08-10 14:58:50, 2018-08-12 01:35:50, 2018-08-17 14:46:00, 2018-09-04 00:21:03, 2018-09-14 14:46:19, 2018-10-09 15:00:24, 2018-10-12 01:47:50, 2018-10-14 22:41:36, 2018-10-20 16:55:56, 2018-10-21 15:57:17, 2018-11-02 13:58:11, 2018-11-03 21:20:42, 2018-11-13 01:41:51, 2018-11-17 16:47:40, 2018-11-24 15:37:00, 2018-12-06 16:34:39, 2018-12-06 16:41:38, 2018-12-06 20:00:28, 2018-12-13 16:03:52, 2018-12-14 00:48:23, 2018-12-16 20:18:14, 2019-01-12 02:58:43, 2019-01-17 16:05:34, 2019-01-23 16:45:30, 2019-02-18 23:17:46, 2019-03-07 23:00:41, 2019-03-13 16:24:02, 2019-03-22 11:43:07, 2019-03-23 01:54:42, 2019-03-26 15:38:49, 2019-04-21 23:41:57, 2019-04-23 15:25:17, 2019-04-23 16:26:22, 2019-05-05 13:19:57, 2019-05-09 03:47:22, 2019-05-10 04:31:13, 2019-05-12 16:18:13, 2019-05-14 15:07:12, 2019-05-30 12:10:22, 2019-06-08 01:58:13, 2019-06-16 18:18:58, 2019-06-18 10:44:48, 2019-06-21 16:39:02, 2019-07-01 12:28:38, 2019-07-05 22:42:03, 2019-07-08 15:40:08, 2019-07-09 15:47:07, 2019-07-12 13:20:06, 2019-07-14 05:11:34, 2019-07-14 08:30:23, 2019-07-19 11:57:35, 2019-07-20 01:38:12, 2019-07-23 15:59:54, 2019-07-26 16:32:25, 2019-08-17 04:10:41, 2019-08-19 23:11:43, 2019-08-22 16:10:38, 2019-08-25 20:52:06, 2019-08-26 14:43:08, 2019-08-30 22:50:23, 2019-08-31 00:23:44, 2019-08-31 04:44:03, 2019-09-06 01:53:00, 2019-09-25 15:56:23, 2019-09-26 17:50:51, 2019-09-27 01:37:06, 2019-10-13 13:32:27, 2019-10-13 13:35:57, 2019-10-13 15:54:51, 2019-10-20 22:44:39, 2019-10-25 14:11:03, 2019-11-03 00:54:27, 2019-11-06 18:30:42, 2019-11-27 14:08:06, 2019-12-15 17:46:08, 2019-12-15 20:21:29, 2019-12-16 14:56:03, 2019-12-20 14:57:05, 2019-12-22 17:10:49, 2019-12-22 17:53:44, 2019-12-28 03:13:54, 2020-01-09 15:55:54, 2020-01-15 02:32:17, 2020-01-19 17:52:22, 2020-01-19 17:52:35, 2020-01-23 02:34:30, 2020-02-02 03:25:05, 2020-02-07 00:29:16, 2020-02-08 18:42:09, 2020-02-13 23:47:04, 2020-02-14 14:09:22, 2020-02-14 15:49:35, 2020-02-17 14:49:54, 2020-02-21 01:22:07, 2020-02-27 12:48:01, 2020-03-09 02:18:25, 2020-03-13 02:42:23, 2020-05-11 14:52:32, 2020-05-20 20:52:34, 2020-07-16 23:47:32, 2020-08-08 14:44:25, 2020-09-04 17:06:14, 2020-09-04 18:04:59, 2020-09-13 13:48:20, 2020-09-20 14:23:52, 2020-10-08 16:40:48, 2020-11-21 14:39:04, 2020-11-29 23:46:58, 2021-01-11 22:52:36, 2021-01-24 00:40:11, 2021-02-20 15:43:04, 2021-02-28 15:18:49, 2021-03-14 11:52:49, 2021-03-20 16:02:55, 2021-03-22 17:59:08, 2021-03-28 15:45:31, 2021-05-03 23:17:00, 2021-06-05 12:38:37, 2021-06-12 00:25:27, 2021-09-11 13:29:47, 2021-09-18 14:56:01, 2021-10-03 14:39:41, 2021-12-19 13:32:27, 2021-12-21 15:41:10, 2021-12-26 15:27:33, 2021-12-31 17:11:20 hasta:  2022-01-19 01:15:21\n"
     ]
    }
   ],
   "source": [
    "print('Datos iniciales desde: ', df_checkin_concatenado['date'].min(), 'hasta: ',df_checkin_concatenado['date'].max())\n",
    "# df_tip_concatenado['date'] = pd.to_datetime(df_tip_concatenado['date'])\n",
    "# df_tip_2018_to_2022 = df_tip_concatenado[df_tip_concatenado['date'].dt.year.between(2018, 2022)]\n",
    "# df_tip_2018_to_2022 = df_tip_2018_to_2022.reset_index(drop=True)\n",
    "# df_tip_2018_to_2022['date'] = pd.to_datetime(df_tip_2018_to_2022['date']).dt.strftime('%Y-%m-%d')\n",
    "# print('Datos finales desde: ', df_tip_2018_to_2022['date'].min(), 'hasta: ',df_tip_2018_to_2022['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_checkin_concatenado)                              # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "nombre_archivo = \"checkin-reducido.parquet\"                                       # Se arma el nombre del archivo\n",
    "ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/checkin/', nombre_archivo) # La ruta donde se guardara el archivo\n",
    "pq.write_table(table, ruta_archivo)                                               # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a limitaciones del repositorio de GitHob, donde solo permite archivos de tamaño maximo de 100MB, se procede a generar archivos que no excedan dicho tamaño.<br>\n",
    ". Cantidad total de registros:  131930<br>\n",
    ". Cantidad de registros aproximado por archivos: 32982<br>\n",
    ". Cantidad de archivos: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_checkin_parquet = \"../Datasets/Datasets_Yelp/checkin/checkin-reducido.parquet\"  # Ruta y nombre del Archivo original\n",
    "datos_checkin_parquet = pq.read_table(archivo_checkin_parquet)                          # Se lee y se carga el archivo como una tabla\n",
    "cantidad_registros = 32982                                                              # Cantidad de registros por archivos separados\n",
    "for ind in range(0, datos_checkin_parquet.num_rows, cantidad_registros):                # Un ciclo para grupo de registros\n",
    "    info_cantidad_registros = datos_checkin_parquet.slice(ind, cantidad_registros)      # Se extrae grupos de cantidad_registros registros\n",
    "    nombre_archivo = \"checkin-reducido_\" + str(ind // cantidad_registros) + \".parquet\"  # Se arma el nombre del archivo\n",
    "    ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/checkin/', nombre_archivo)   # La ruta donde se guardara el archivo\n",
    "    pq.write_table(info_cantidad_registros, ruta_archivo)                               # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando los archivos parquet reducidos como un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/checkin/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"checkin-reducido_*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_checkin = pq.read_table(source=archivo)   # Se crea una tabla con cada archivo\n",
    "    df = tabla_checkin.to_pandas()                  # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_checkin_reducido_concatenado = pd.concat(df_list)# Se concatenan todos los DataFrames en uno solo\n",
    "df_checkin_reducido_concatenado = df_checkin_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kPU91CF4Lq2WlRu9Lw</td>\n",
       "      <td>2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0iUa4sNDFiZFrAdIWhZQ</td>\n",
       "      <td>2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30_8IhuyMHbSOcNWd6DQ</td>\n",
       "      <td>2013-06-14 23:29:17, 2014-08-13 23:20:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7jw19RH9JKXgFohspgQw</td>\n",
       "      <td>2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            business_id                                               date\n",
       "0    kPU91CF4Lq2WlRu9Lw  2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
       "1  0iUa4sNDFiZFrAdIWhZQ  2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
       "2  30_8IhuyMHbSOcNWd6DQ           2013-06-14 23:29:17, 2014-08-13 23:20:22\n",
       "3  7PUidqRWpRSpXebiyxTg  2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
       "4  7jw19RH9JKXgFohspgQw  2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_checkin_reducido_concatenado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se le quita el caracter '-' del campo business_id\n",
    "df_checkin_reducido_concatenado['business_id'] = df_checkin_reducido_concatenado['business_id'].str.replace('-','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkin_reducido_concatenado = df_checkin_reducido_concatenado.explode('date')\n",
    "\n",
    "# df_checkin_reducido_concatenado.date.str.split(pat=',',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
      "1         2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
      "2                  2013-06-14 23:29:17, 2014-08-13 23:20:22\n",
      "3         2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
      "4         2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...\n",
      "                                ...                        \n",
      "131925    2013-03-23 16:22:47, 2013-04-07 02:03:12, 2013...\n",
      "131926                                  2021-06-12 01:16:12\n",
      "131927    2011-05-24 01:35:13, 2012-01-01 23:44:33, 2012...\n",
      "131928             2016-12-03 23:33:26, 2018-12-02 19:08:45\n",
      "131929                                  2015-01-06 17:51:53\n",
      "Name: date, Length: 131930, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_checkin_reducido_concatenado['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda un dataframe en formato parquet\n",
    "archivo_checkin_parquet = \"../Datasets/Datasets_Yelp/checkin/checkin-reducido.parquet\"\n",
    "df_checkin_reducido_concatenado.to_parquet(archivo_checkin_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: tip`\n",
    "\n",
    "Tipo de archivo JSon\n",
    "\n",
    "Procedimieto que permite separar el dataset tip en 1 archivos tipo parquet.<br>\n",
    "El datasets tip tiene aproximadamente: 908.915 registros.<br>\n",
    "Se abre el archivo JSon y se salva tipo parquet con la misma cantidad de registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_tip_json = \"../Datsets/Datasets_Yelp/tip/review.json\"\n",
    "\n",
    "with jsonlines.open(archivo_tip_json) as reader:\n",
    "    lista_datos = list(reader)                                                  # Lee todos los registros y crea una lista\n",
    "    total_registros = len(lista_datos)                                          # Se tiene el total de registros, esto para separar en varios archivos.\n",
    "\n",
    "    for ind in range(0, total_registros, total_registros):\n",
    "        iformacion_registros = lista_datos[ind:ind+total_registros]             # Se extrae grupos de cantidad_registros registros\n",
    "        df = pd.DataFrame(iformacion_registros)                                 # Se crea un dataframe con el grupo de registros extraidos\n",
    "        table = pa.Table.from_pandas(df)                                        # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "        nombre_archivo = \"tip_\" + str(ind//total_registros) + \".parquet\"        # Se arma el nombre del archivo\n",
    "        ruta_archivo = os.path.join('../Datsets/Datasets_Yelp/tip/', nombre_archivo)    # La ruta donde se guardara el archivo\n",
    "        pq.write_table(table, ruta_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/tip/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"tip_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_tip_concatenado = pd.concat(df_list)        # Se concatenan todos los DataFrames en uno solo\n",
    "df_tip_concatenado = df_tip_concatenado.reset_index(drop=True)    # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario del dataset tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Column | Dtype | RealType |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| 0 | user_id | object | string |\n",
    "| 1 | business_id | object | string |\n",
    "| 2 | text | object | string |\n",
    "| 3 | date | float64 | date YYYY-MM-DD |\n",
    "| 4 | compliment_count | int64 | int64 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su manipulacion se reduce el dataset a 5 años de registros y se ajustan los campos:<br> \n",
    ". date al formato: YYYY-MM-DD<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales desde:  2009-04-16 13:11:49 hasta:  2022-01-19 20:38:55\n",
      "Datos finales desde:  2018-01-01 hasta:  2022-01-19\n"
     ]
    }
   ],
   "source": [
    "print('Datos iniciales desde: ', df_tip_concatenado['date'].min(), 'hasta: ',df_tip_concatenado['date'].max())\n",
    "df_tip_concatenado['date'] = pd.to_datetime(df_tip_concatenado['date'])\n",
    "df_tip_2018_to_2022 = df_tip_concatenado[df_tip_concatenado['date'].dt.year.between(2018, 2022)]\n",
    "df_tip_2018_to_2022 = df_tip_2018_to_2022.reset_index(drop=True)\n",
    "df_tip_2018_to_2022['date'] = pd.to_datetime(df_tip_2018_to_2022['date']).dt.strftime('%Y-%m-%d')\n",
    "print('Datos finales desde: ', df_tip_2018_to_2022['date'].min(), 'hasta: ',df_tip_2018_to_2022['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un archivo en el directorio tip con el dataset reducido en cantidad de registros para 5 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_tip_2018_to_2022)                              # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "nombre_archivo = \"tip-reducido.parquet\"                                        # Se arma el nombre del archivo\n",
    "ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/tip/', nombre_archivo)  # La ruta donde se guardara el archivo\n",
    "pq.write_table(table, ruta_archivo)                                            # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "2 años de registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/tip/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"tip-reducido*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_review = pq.read_table(source=archivo)    # Se crea una tabla con cada archivo\n",
    "    df = tabla_review.to_pandas()                   # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_tip_reducido_concatenado = pd.concat(df_list) # Se concatenan todos los DataFrames en uno solo\n",
    "df_tip_reducido_concatenado = df_tip_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales desde:  2018-01-01 00:00:00 hasta:  2022-01-19 00:00:00\n",
      "Datos finales desde:  2018-01-01 hasta:  2020-12-31\n"
     ]
    }
   ],
   "source": [
    "print('Datos iniciales desde: ', df_tip_reducido_concatenado['date'].min(), 'hasta: ',df_tip_reducido_concatenado['date'].max())\n",
    "df_tip_reducido_concatenado['date'] = pd.to_datetime(df_tip_reducido_concatenado['date'])\n",
    "df_tip_2018_to_2020 = df_tip_reducido_concatenado[df_tip_reducido_concatenado['date'].dt.year.between(2018, 2020)]\n",
    "df_tip_2018_to_2020 = df_tip_2018_to_2020.reset_index(drop=True)\n",
    "df_tip_2018_to_2020['date'] = pd.to_datetime(df_tip_2018_to_2020['date']).dt.strftime('%Y-%m-%d')\n",
    "print('Datos finales desde: ', df_tip_2018_to_2020['date'].min(), 'hasta: ',df_tip_2018_to_2020['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1viszljzsa8W7VnYPuWosw</td>\n",
       "      <td>Zg83olSjsdXAhE5EEy5IcQ</td>\n",
       "      <td>Order crispy and they bake it up good.</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YnlCpuaBa3qWBp4te8pGmA</td>\n",
       "      <td>XIKYdKWq72zUYsq8NBxcCQ</td>\n",
       "      <td>The honey glazed salmon is amazing!</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XJDLaoN1PerKw2woiKeepA</td>\n",
       "      <td>t0zwddmbOGQOADrAxEPHQQ</td>\n",
       "      <td>Come by for some Jelly Bean sours this weekend!</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9M_nk0kA3CTJt6UoxRAK_g</td>\n",
       "      <td>oJf3IZloLf_PDeEeZW5nDg</td>\n",
       "      <td>Meatball sandwich</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0LY6l9-Z2IrVtnfj6OX06w</td>\n",
       "      <td>Zmwm6d872C8kWJr4b6UGfg</td>\n",
       "      <td>Great tasty food</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "0  1viszljzsa8W7VnYPuWosw  Zg83olSjsdXAhE5EEy5IcQ   \n",
       "1  YnlCpuaBa3qWBp4te8pGmA  XIKYdKWq72zUYsq8NBxcCQ   \n",
       "2  XJDLaoN1PerKw2woiKeepA  t0zwddmbOGQOADrAxEPHQQ   \n",
       "3  9M_nk0kA3CTJt6UoxRAK_g  oJf3IZloLf_PDeEeZW5nDg   \n",
       "4  0LY6l9-Z2IrVtnfj6OX06w  Zmwm6d872C8kWJr4b6UGfg   \n",
       "\n",
       "                                              text        date  \\\n",
       "0           Order crispy and they bake it up good.  2018-04-01   \n",
       "1              The honey glazed salmon is amazing!  2018-01-14   \n",
       "2  Come by for some Jelly Bean sours this weekend!  2018-03-31   \n",
       "3                                Meatball sandwich  2018-03-27   \n",
       "4                                 Great tasty food  2018-04-01   \n",
       "\n",
       "   compliment_count  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tip_reducido_concatenado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a limitaciones del repositorio de GitHob, donde solo permite archivos de tamaño maximo de 100MB.\n",
    "En el caso del dataset tip, no es necesario separar en varios archivos ya que su tamaño se ajusta.<br>\n",
    ". Cantidad total de registros:  193033<br>\n",
    ". Cantidad de archivos: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando los archivos parquet reducidos como un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/tip/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"tip-reducido*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_review = pq.read_table(source=archivo)    # Se crea una tabla con cada archivo\n",
    "    df = tabla_review.to_pandas()                   # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_tip_reducido_concatenado = pd.concat(df_list) # Se concatenan todos los DataFrames en uno solo\n",
    "df_tip_reducido_concatenado = df_tip_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1viszljzsa8W7VnYPuWosw</td>\n",
       "      <td>Zg83olSjsdXAhE5EEy5IcQ</td>\n",
       "      <td>Order crispy and they bake it up good.</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YnlCpuaBa3qWBp4te8pGmA</td>\n",
       "      <td>XIKYdKWq72zUYsq8NBxcCQ</td>\n",
       "      <td>The honey glazed salmon is amazing!</td>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XJDLaoN1PerKw2woiKeepA</td>\n",
       "      <td>t0zwddmbOGQOADrAxEPHQQ</td>\n",
       "      <td>Come by for some Jelly Bean sours this weekend!</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9M_nk0kA3CTJt6UoxRAK_g</td>\n",
       "      <td>oJf3IZloLf_PDeEeZW5nDg</td>\n",
       "      <td>Meatball sandwich</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0LY6l9-Z2IrVtnfj6OX06w</td>\n",
       "      <td>Zmwm6d872C8kWJr4b6UGfg</td>\n",
       "      <td>Great tasty food</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "0  1viszljzsa8W7VnYPuWosw  Zg83olSjsdXAhE5EEy5IcQ   \n",
       "1  YnlCpuaBa3qWBp4te8pGmA  XIKYdKWq72zUYsq8NBxcCQ   \n",
       "2  XJDLaoN1PerKw2woiKeepA  t0zwddmbOGQOADrAxEPHQQ   \n",
       "3  9M_nk0kA3CTJt6UoxRAK_g  oJf3IZloLf_PDeEeZW5nDg   \n",
       "4  0LY6l9-Z2IrVtnfj6OX06w  Zmwm6d872C8kWJr4b6UGfg   \n",
       "\n",
       "                                              text        date  \\\n",
       "0           Order crispy and they bake it up good.  2018-04-01   \n",
       "1              The honey glazed salmon is amazing!  2018-01-14   \n",
       "2  Come by for some Jelly Bean sours this weekend!  2018-03-31   \n",
       "3                                Meatball sandwich  2018-03-27   \n",
       "4                                 Great tasty food  2018-04-01   \n",
       "\n",
       "   compliment_count  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tip_reducido_concatenado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: review`\n",
    "\n",
    "Tipo de archivo JSon\n",
    "\n",
    "Procedimieto que permite separar el dataset review en 13 archivos tipo parquet con aproximadamente 500.000 registros cada archivo.<br>\n",
    "El datasets reviews tiene aproximadamente: 6.990.280 registros.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente procedimiento lee el datasets original JSon, aproximadamente de 5Gb y lo separa en archivos tipo parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_review_json = \"../Datsets/Datasets_Yelp/review/review.json\"             # Ruta y nombre del Archivo original\n",
    "cantidad_registros = 500000                                                     # Cantidad de registros por archivos\n",
    "with jsonlines.open(archivo_review_json) as reader:                             # Se lee el archivo original JSon\n",
    "    lista_datos = list(reader)                                                  # Lee todos los registros y crea una lista\n",
    "    total_registros = len(lista_datos)                                          # Se tiene el total de registros, esto para separar en varios archivos.\n",
    "    for ind in range(0, total_registros, cantidad_registros):                   # Un ciclo por cada grupo de registors\n",
    "        iformacion_registros = lista_datos[ind:ind+cantidad_registros]          # Se extrae grupos de cantidad_registros registros\n",
    "        df = pd.DataFrame(iformacion_registros)                                 # Se crea un dataframe con el grupo de registros extraidos\n",
    "        table = pa.Table.from_pandas(df)                                        # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "        nombre_archivo = \"review_\" + str(ind//cantidad_registros) + \".parquet\"  # Se arma el nombre del archivo\n",
    "        ruta_archivo = os.path.join('../Datsets/Datasets_Yelp/review/', nombre_archivo) # La ruta donde se guardara el archivo\n",
    "        pq.write_table(table, ruta_archivo)                                     # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/review/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"review_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_review_concatenado = pd.concat(df_list)        # Se concatenan todos los DataFrames en uno solo\n",
    "df_review_concatenado = df_review_concatenado.reset_index(drop=True)    # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario del dataset review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Column | Dtype | RealType |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| 0 | review_id | object | string |\n",
    "| 1 | user_id | object | string |\n",
    "| 2 | business_id | object | string |\n",
    "| 3 | stars | float64 | int64 |\n",
    "| 4 | useful | int64 | int64 |\n",
    "| 5 | funny | int64 | int64 |\n",
    "| 6 | cool | int64 | int64 |\n",
    "| 7 | text | object | string |\n",
    "| 8 | date | object | date YYYY-MM-DD |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su manipulacion se reduce el dataset a 5 años de registros y se ajustan los campos:<br> \n",
    ". date al formato: YYYY-MM-DD<br>\n",
    ". starts a tipo int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales desde:  2005-02-16 03:23:22 hasta:  2022-01-19 19:48:45\n",
      "Datos finales desde:  2018-01-01 hasta:  2022-01-19\n"
     ]
    }
   ],
   "source": [
    "print('Datos iniciales desde: ', df_review_concatenado['date'].min(), 'hasta: ',df_review_concatenado['date'].max())\n",
    "df_review_concatenado['date'] = pd.to_datetime(df_review_concatenado['date'])\n",
    "df_review_2018_to_2022 = df_review_concatenado[df_review_concatenado['date'].dt.year.between(2018, 2022)]\n",
    "df_review_2018_to_2022 = df_review_2018_to_2022.reset_index(drop=True)\n",
    "df_review_2018_to_2022['date'] = pd.to_datetime(df_review_2018_to_2022['date']).dt.strftime('%Y-%m-%d')\n",
    "df_review_2018_to_2022['stars'] = df_review_2018_to_2022['stars'].astype('int64')\n",
    "print('Datos finales desde: ', df_review_2018_to_2022['date'].min(), 'hasta: ',df_review_2018_to_2022['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un archivo en el directorio review con el dataset reducido en cantidad de registros para 5 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_review_2018_to_2022)                              # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "nombre_archivo = \"review-reducido.parquet\"                                        # Se arma el nombre del archivo\n",
    "ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/review/', nombre_archivo)  # La ruta donde se guardara el archivo\n",
    "pq.write_table(table, ruta_archivo)                                               # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a limitaciones del repositorio de GitHob, donde solo permite archivos de tamaño maximo de 100MB, se procede a generar archivos que no excedan dicho tamaño.<br>\n",
    ". Cantidad total de registros:  2742210<br>\n",
    ". Cantidad de registros aproximado por archivos: 100000<br>\n",
    ". Cantidad de archivos: 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedimiento para crear archivos separados de grupos de registros donde el tamaño de cada archivo no exceda de 50Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_review_parquet = \"../Datasets/Datasets_Yelp/review/review-reducido.parquet\" # Ruta y nombre del Archivo original\n",
    "datos_review_parquet = pq.read_table(archivo_review_parquet)                        # Se lee y se carga el archivo como una tabla\n",
    "cantidad_registros = 100000                                                         # Cantidad de registros por archivos separados\n",
    "for ind in range(0, datos_review_parquet.num_rows, cantidad_registros):             # Un ciclo para grupo de registros\n",
    "    info_cantidad_registros = datos_review_parquet.slice(ind, cantidad_registros)   # Se extrae grupos de cantidad_registros registros\n",
    "    nombre_archivo = \"review-reducido_\" + str(ind // cantidad_registros) + \".parquet\"   # Se arma el nombre del archivo\n",
    "    ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/review/', nombre_archivo)    # La ruta donde se guardara el archivo\n",
    "    pq.write_table(info_cantidad_registros, ruta_archivo)                           # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando los archivos parquet reducidos como un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/review/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"review-reducido_*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_review = pq.read_table(source=archivo)    # Se crea una tabla con cada archivo\n",
    "    df = tabla_review.to_pandas()                   # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_review_reducido_concatenado = pd.concat(df_list) # Se concatenan todos los DataFrames en uno solo\n",
    "df_review_reducido_concatenado = df_review_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2742210, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_reducido_concatenado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se le quita la columna funny\n",
    "df_review_reducido_concatenado =  df_review_reducido_concatenado.drop('funny', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>lUUhg8ltDsUZ9h0xnwY4Dg</td>\n",
       "      <td>JBWZmBy69VMggxj3eYn17Q</td>\n",
       "      <td>E9AB7V4z8xrt2uPF7T55FQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>RreNy--tOmXMl1en0wiBOg</td>\n",
       "      <td>aFa96pz67TwOFu4Weq5Agg</td>\n",
       "      <td>iYY5Ii1LGpZCpXFkHlMefw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>cPepkJeRMtHapc_b2Oe_dw</td>\n",
       "      <td>kq5Ghhh14r-eCxlVmlyd8w</td>\n",
       "      <td>Zx7n8mdt8OzLRXVzolXNhQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>I was really between 3 and 4 stars for this on...</td>\n",
       "      <td>My boyfriend and I tried this deli for the fir...</td>\n",
       "      <td>Amazing biscuits and (fill in the blank). Grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>2018-07-17</td>\n",
       "      <td>2018-08-23</td>\n",
       "      <td>2018-04-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0  \\\n",
       "review_id                               KU_O5udG6zpxOg-VcAEodg   \n",
       "user_id                                 mh_-eMZ6K5RLWhZyISBhwA   \n",
       "business_id                             XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "stars                                                        3   \n",
       "useful                                                       0   \n",
       "cool                                                         0   \n",
       "text         If you decide to eat here, just be aware it is...   \n",
       "date                                                2018-07-07   \n",
       "\n",
       "                                                             1  \\\n",
       "review_id                               lUUhg8ltDsUZ9h0xnwY4Dg   \n",
       "user_id                                 RreNy--tOmXMl1en0wiBOg   \n",
       "business_id                             cPepkJeRMtHapc_b2Oe_dw   \n",
       "stars                                                        4   \n",
       "useful                                                       1   \n",
       "cool                                                         1   \n",
       "text         I was really between 3 and 4 stars for this on...   \n",
       "date                                                2018-07-17   \n",
       "\n",
       "                                                             2  \\\n",
       "review_id                               JBWZmBy69VMggxj3eYn17Q   \n",
       "user_id                                 aFa96pz67TwOFu4Weq5Agg   \n",
       "business_id                             kq5Ghhh14r-eCxlVmlyd8w   \n",
       "stars                                                        5   \n",
       "useful                                                       0   \n",
       "cool                                                         0   \n",
       "text         My boyfriend and I tried this deli for the fir...   \n",
       "date                                                2018-08-23   \n",
       "\n",
       "                                                             3  \n",
       "review_id                               E9AB7V4z8xrt2uPF7T55FQ  \n",
       "user_id                                 iYY5Ii1LGpZCpXFkHlMefw  \n",
       "business_id                             Zx7n8mdt8OzLRXVzolXNhQ  \n",
       "stars                                                        5  \n",
       "useful                                                       0  \n",
       "cool                                                         0  \n",
       "text         Amazing biscuits and (fill in the blank). Grea...  \n",
       "date                                                2018-04-27  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_reducido_concatenado.head(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un archivo en formato csv\n",
    "\n",
    "archivo_review_csv = \"../Datasets/Datasets_Yelp/review/review.csv\"\n",
    "df_review_reducido_concatenado.to_csv(archivo_review_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_review_csv = \"../Datasets/Datasets_Yelp/review/review.csv\"\n",
    "df = pd.read_csv(archivo_review_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>lUUhg8ltDsUZ9h0xnwY4Dg</td>\n",
       "      <td>JBWZmBy69VMggxj3eYn17Q</td>\n",
       "      <td>E9AB7V4z8xrt2uPF7T55FQ</td>\n",
       "      <td>A4n4YaE-owOVgTQcrVqHUw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>RreNy--tOmXMl1en0wiBOg</td>\n",
       "      <td>aFa96pz67TwOFu4Weq5Agg</td>\n",
       "      <td>iYY5Ii1LGpZCpXFkHlMefw</td>\n",
       "      <td>S7bjj-L07JuRr-tpX1UZLw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>cPepkJeRMtHapc_b2Oe_dw</td>\n",
       "      <td>kq5Ghhh14r-eCxlVmlyd8w</td>\n",
       "      <td>Zx7n8mdt8OzLRXVzolXNhQ</td>\n",
       "      <td>I6L0Zxi5Ww0zEWSAVgngeQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>I was really between 3 and 4 stars for this on...</td>\n",
       "      <td>My boyfriend and I tried this deli for the fir...</td>\n",
       "      <td>Amazing biscuits and (fill in the blank). Grea...</td>\n",
       "      <td>The cafe was extremely cute. We came at 8am an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>2018-07-17</td>\n",
       "      <td>2018-08-23</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>2018-07-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0  \\\n",
       "review_id                               KU_O5udG6zpxOg-VcAEodg   \n",
       "user_id                                 mh_-eMZ6K5RLWhZyISBhwA   \n",
       "business_id                             XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "stars                                                        3   \n",
       "useful                                                       0   \n",
       "cool                                                         0   \n",
       "text         If you decide to eat here, just be aware it is...   \n",
       "date                                                2018-07-07   \n",
       "\n",
       "                                                             1  \\\n",
       "review_id                               lUUhg8ltDsUZ9h0xnwY4Dg   \n",
       "user_id                                 RreNy--tOmXMl1en0wiBOg   \n",
       "business_id                             cPepkJeRMtHapc_b2Oe_dw   \n",
       "stars                                                        4   \n",
       "useful                                                       1   \n",
       "cool                                                         1   \n",
       "text         I was really between 3 and 4 stars for this on...   \n",
       "date                                                2018-07-17   \n",
       "\n",
       "                                                             2  \\\n",
       "review_id                               JBWZmBy69VMggxj3eYn17Q   \n",
       "user_id                                 aFa96pz67TwOFu4Weq5Agg   \n",
       "business_id                             kq5Ghhh14r-eCxlVmlyd8w   \n",
       "stars                                                        5   \n",
       "useful                                                       0   \n",
       "cool                                                         0   \n",
       "text         My boyfriend and I tried this deli for the fir...   \n",
       "date                                                2018-08-23   \n",
       "\n",
       "                                                             3  \\\n",
       "review_id                               E9AB7V4z8xrt2uPF7T55FQ   \n",
       "user_id                                 iYY5Ii1LGpZCpXFkHlMefw   \n",
       "business_id                             Zx7n8mdt8OzLRXVzolXNhQ   \n",
       "stars                                                        5   \n",
       "useful                                                       0   \n",
       "cool                                                         0   \n",
       "text         Amazing biscuits and (fill in the blank). Grea...   \n",
       "date                                                2018-04-27   \n",
       "\n",
       "                                                             4  \n",
       "review_id                               A4n4YaE-owOVgTQcrVqHUw  \n",
       "user_id                                 S7bjj-L07JuRr-tpX1UZLw  \n",
       "business_id                             I6L0Zxi5Ww0zEWSAVgngeQ  \n",
       "stars                                                        4  \n",
       "useful                                                       0  \n",
       "cool                                                         0  \n",
       "text         The cafe was extremely cute. We came at 8am an...  \n",
       "date                                                2018-07-07  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando el archivo parquet como una table pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_review_parquet = \"../Datasets/Datasets_Yelp/review/review-reducido.parquet\"\n",
    "datos_tabla_parquet = pq.read_table(archivo_review_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de dato:  <class 'pyarrow.lib.Table'>\n",
      "Tipos de datos:  pyarrow.Table\n",
      "review_id: string\n",
      "user_id: string\n",
      "business_id: string\n",
      "stars: double\n",
      "useful: int64\n",
      "funny: int64\n",
      "cool: int64\n",
      "text: string\n",
      "date: string\n",
      "----\n",
      "review_id: [[\"KU_O5udG6zpxOg-VcAEodg\",\"BiTunyQ73aT9WBnpR9DZGw\",\"saUsX_uimxRlCVr67Z4Jig\",\"AqPFMleE6RsU23_auESxiA\",\"Sx8TMOWLNuJBWer-0pcmoA\",\"JrIxlS1TzJ-iCu79ul40cQ\",\"6AxgBCNX_PNTOxmbRSwcKQ\",\"_ZeMknuYdlQcUqng_Im3yg\",\"ZKvDG2sBvHVdF5oBNUOpAQ\",\"pUycOfUwM8vqX7KjRRhUEA\",...,\"VX_qZ3uK7SIomq1t4BzFnQ\",\"aFPIbN3aj5pN0RmlUDkmgw\",\"Ki7ncLCgVWK9Ysukwk0zJg\",\"lJqnUK9gASgdyx9USoKzfw\",\"W8I3wWhb9hmglasrR6dePg\",\"q4Z6NCR1IMRdpPorTD89Tg\",\"KTg0i04SbVWGWHHJmGOI_A\",\"ZuMNAPcArFtaGufe-nwGOA\",\"Lohri9uZyvoNnH5z_NT6DA\",\"9BaxxfWBmIsSPlx-kvRBDw\"]]\n",
      "user_id: [[\"mh_-eMZ6K5RLWhZyISBhwA\",\"OyoGAe7OKpv6SyGZT5g77Q\",\"8g_iMtfSiwikVnbP2etR0A\",\"_7bHUi9Uuf5__HHc_Q8guQ\",\"bcjbaE6dDog4jkNY91ncLQ\",\"eUta8W_HdHMXPzLBBZhL1A\",\"r3zeYsv1XFBRA4dJpL78cw\",\"yfFzsLmaWF2d4Sr0UNbBgg\",\"wSTuiTk-sKNdcFyprzZAjg\",\"59MxRhNVhU9MYndMkz0wtw\",...,\"8TY9eLPAYyvwrqWFx77HvA\",\"r_jYtiPrYxXHDlEZCgJsTw\",\"00hTIAj8PYj5vbaVtbpZJw\",\"7N8IMmMrevDQ0B_Hqa87pQ\",\"MZlZfs0ro3Aj3-FPOBPE-Q\",\"tCPzvPwsd_DwBx6whsDlLQ\",\"rj0uSTXu1rPVgAfOMHYltQ\",\"MVqzYt-E7y1Mdw9F_7nLjw\",\"vn777Y2vCynYYUYJjNYdYg\",\"_49VYA-2qlNOOkE8Hc7Z_A\"]]\n",
      "business_id: [[\"XQfwVwDr-v0ZS3_CbbE5Xw\",\"7ATYjTIgM3jUlt4UM3IypQ\",\"YjUWPpI6HXG530lwP-fb2A\",\"kxX2SOes4o-D3ZQBkiMRfA\",\"e4Vwtrqf-wpJfwesgvdgxQ\",\"04UD14gamNjLY0IDYVhHJg\",\"gmjsEdUsKpj9Xxu6pdjH0g\",\"LHSTtnW3YHCeUkRDGyJOyw\",\"B5XSoSG3SfvQGtKEGQ1tSQ\",\"gebiRewfieSdtt17PTW6Zg\",...,\"xT41eqUSsM_MDE1Vjhq20A\",\"AHAPYD1ADG6MD8RB1LprWA\",\"itAhmbhHOyQQparfwicjDQ\",\"iEC0VzHM2hQ13wM7c0QwgQ\",\"DMmdMkh7sIhUHgVawGxY6g\",\"-QwHN9KoluPcA0YFllwFYQ\",\"t_v2TyjeqaRkrfZKudY9cA\",\"3XirYkP9PJvVXIEDPNNXLA\",\"gfPDLZimZu1NtBIDbeXetg\",\"sWCCxY1-9B1FGlSVeQvnHg\"]]\n",
      "stars: [[3,5,3,5,4,1,5,5,3,3,...,5,1,5,4,5,5,5,4,2,5]]\n",
      "useful: [[0,1,0,1,1,1,0,2,1,0,...,0,6,1,0,0,0,0,0,1,0]]\n",
      "funny: [[0,0,0,0,0,2,2,0,1,0,...,0,5,0,0,0,0,0,0,0,0]]\n",
      "cool: [[0,1,0,1,1,1,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]]\n",
      "text: [[\"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \n",
      "\n",
      "The food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\",\"I've taken a lot of spin classes over the years, and nothing compares to the classes at Body Cycle. From the nice, clean space and amazing bikes, to the welcoming and motivating instructors, every class is a top notch work out.\n",
      "\n",
      "For anyone who struggles to fit workouts in, the online scheduling system makes it easy to plan ahead (and there's no need to line up way in advanced like many gyms make you do).\n",
      "\n",
      "There is no way I can write this review without giving Russell, the owner of Body Cycle, a shout out. Russell's passion for fitness and cycling is so evident, as is his desire for all of his clients to succeed. He is always dropping in to classes to check in/provide encouragement, and is open to ideas and recommendations from anyone. Russell always wears a smile on his face, even when he's kicking your butt in class!\",\"Family diner. Had the buffet. Eclectic assortment: a large chicken leg, fried jalapeño, tamale, two rolled grape leaves, fresh melon. All good. Lots of Mexican choices there. Also has a menu with breakfast served all day long. Friendly, attentive staff. Good place for a casual relaxed meal with no expectations. Next to the Clarion Hotel.\",\"Wow!  Yummy, different,  delicious.   Our favorite is the lamb curry and korma.  With 10 different kinds of naan!!!  Don't let the outside deter you (because we almost changed our minds)...go in and try something new!   You'll be glad you did!\",\"Cute interior and owner (?) gave us tour of upcoming patio/rooftop area which will be great on beautiful days like today. Cheese curds were very good and very filling. Really like that sandwiches come w salad, esp after eating too many curds! Had the onion, gruyere, tomato sandwich. Wasn't too much cheese which I liked. Needed something else...pepper jelly maybe. Would like to see more menu options added such as salads w fun cheeses. Lots of beer and wine as well as limited cocktails. Next time I will try one of the draft wines.\",\"I am a long term frequent customer of this establishment. I just went in to order take out (3 apps) and was told they're too busy to do it. Really? The place is maybe half full at best. Does your dick reach your ass? Yes? Go fuck yourself! I'm a frequent customer AND great tipper. Glad that Kanella just opened. NEVER going back to dmitris!\",\"Loved this tour! I grabbed a groupon and the price was great. It was the perfect way to explore New Orleans for someone who'd never been there before and didn't know a lot about the history of the city. Our tour guide had tons of interesting tidbits about the city, and I really enjoyed the experience. Highly recommended tour. I actually thought we were just going to tour through the cemetery, but she took us around the French Quarter for the first hour, and the cemetery for the second half of the tour. You'll meet up in front of a grocery store (seems strange at first, but it's not terribly hard to find, and it'll give you a chance to get some water), and you'll stop at a visitor center part way through the tour for a bathroom break if needed. This tour was one of my favorite parts of my trip!\",\"Amazingly amazing wings and homemade bleu cheese. Had the ribeye: tender, perfectly prepared, delicious. Nice selection of craft beers. Would DEFINITELY recommend checking out this hidden gem.\",\"This easter instead of going to Lopez Lake we went to Los Padres National Forest which is really pretty but if you go to white rock the staff needs to cut down all the dead grass that invades the rock and the water. I would wish the staff would also clean or get rid of the dead grass that's also living by the water. The water is really green and dirty. Los padres national forest staff need to work hard to maintain this forest looking pretty and not like a dumpster. Even Cachuma lake looks like they put a bit more effort.\",\"Had a party of 6 here for hibachi. Our waitress brought our separate sushi orders on one plate so we couldn't really tell who's was who's and forgot several items on an order. I understand making mistakes but the restaraunt was really quiet so we were kind of surprised. Usually hibachi is a fun lively experience and our  cook  said maybe three words, but he cooked very well his name was Francisco. Service was fishy, food was pretty good, and im hoping it was just an off night here. But for the money I wouldn't go back.\",...,\"The standard wrap sandwich at this store was delicious! The extra $5 for the Gulf Shrimp  was well worth it!\",\"I asked if there was a minimum about of hours you had to rent it for a Saturday night and this was the response I received    \n",
      "\n",
      "\"While you MAY be able to reserve a request for as few as three hours on a Saturday night, there is a strong chance that on VERY LITTLE notice ( an hour or two ) you will receive a phone call explaining that the vehicle was \"damaged, delayed and / or defunct\" and that no other vehicle is available. This last-minute cancellation will be the result of a better request  ( $$$ ) for the company. While Profile Transportation will accept a 3-hour booking ( perhaps a day or two in advance of service ), I am not able to commit a vehicle on a Saturday night two-plus months in advance.  Please remember that you need a vehicle which is specialized...not the standard sedan or SUV. Be careful. Thank you. Philip.\".\",\"Delicious coffee and wonderful little shop in the warehouse district. The home made oat milk was killer. Loved it and made my latte smooth. No need for sugar. I can't wait to try the homemade almond milk tomorrow.\",\"Loved the Monte Cristo sandwich. Previously gave them 5 stars! Now that the pandemic is slowing and they are back to a full menu, they took the Monte Cristo off the menu. It was good whike it lasted. So sorry to see it go. The only other meal I like there is the eggs benedict. So, I guess I won't be going to Metro very ofyen.\",\"This is the best BBQ in the area. The owners are super friendly, the prices are reasonable, the sandwiches are LOADED. Great meats and sides. They catered a party for me and everyone loved with the ribs and pulled pork. Don't be turned off by the setting (being at a gas station), give them a chance and you too will be standing in line for some.\",\"We won the Playoff game last night and is scheduled for the Championship game tonight! Out Juniata Bears 11/12 are winners no matter what happens tonight because they fought hard for this opportunity. We wanted to reward our team with trophies to show they're winners no matter the outcome !. Everyone we called told us we needed 24 hours' notice so it couldn't be done. When I called Spike's Trophies, they heard the desperation in my voice and took my order without hesitation. I will forever be grateful to Lisa and Ted who answered the phone. She took her time making sure my needs were met. I'm beyond excited for the kids.\n",
      "\n",
      "I would highly recommend this business and will use their services from now on!\n",
      "\n",
      "A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A+A\",\"We have been a resident almost 4 years at Manzanita and absolutely love it. The setting is peaceful and tranquil. The office and staff here are always helpful. Joanne has been the best in always helping in our details and facilitating all our needs. I recommend living here. It's a community that offers something for everyone\n",
      "\n",
      "\"A place to call home\"\n",
      "\n",
      "Kim & Luis\",\"This place is hyped as one of the best places in St Louis. They are located in an interesting part of town. The history of the story is great. Tried a few chocolates and malt balls which were great. Unfortunately their ice cream and shakes were not that great compared to others we've had.\",\"Saw the reviews so thought I'd try this place. Was really disappointed. The rice was supposed to be basmati but was regular rice and on top of that no flavor at all like no herbs, no hint of taste, sweet or otherwise like at a good Indian restaurant. I got vegetable jalfrezi and the vegetables were the cheap frozen kind of mixed vegetables you get at the supermarket! Really?!? Once I got over my confusion I found at least the sauce was good. Garlic naan was really good and was the best part of the meal, actually the only thing I ate all of. The pics are what I left.... Sorry couldn't eat that. I love Indian food and just couldn't do it to myself. At least when I complained the waitress have me 20%off. But sorry, won't be back. Two stars for the naan.\",\"Yummy!  I ordered Uber Eats and wasn't sure how this was going to go. I ordered the Cajun Shrimp Alfredo.  OMG!  It was delicious!  The portion was huge. I had the meatballs and cheese.  OMG!  It was heaven!  Everything about this place was the BOMB!  My new go-to spot!\"]]\n",
      "date: [[\"2018-07-07 22:09:11\",\"2012-01-03 15:28:18\",\"2014-02-05 20:30:30\",\"2015-01-04 00:01:03\",\"2017-01-14 20:54:15\",\"2015-09-23 23:10:31\",\"2015-01-03 23:21:18\",\"2015-08-07 02:29:16\",\"2016-03-30 22:46:33\",\"2016-07-25 07:31:06\",...,\"2021-06-27 17:23:18\",\"2011-08-08 14:22:39\",\"2021-05-14 17:09:38\",\"2021-06-29 13:48:52\",\"2021-02-02 22:05:03\",\"2021-06-30 16:14:03\",\"2021-03-24 20:21:27\",\"2021-07-03 04:51:07\",\"2019-04-16 23:28:41\",\"2021-07-06 01:58:08\"]]\n",
      "Cantidad de columnas:  9\n",
      "Nombre columnas:  ['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'text', 'date']\n",
      "Cantidad de registros:  500000\n"
     ]
    }
   ],
   "source": [
    "print(\"Tipo de dato: \",type(datos_tabla_parquet))\n",
    "print(\"Tipos de datos: \",datos_tabla_parquet)\n",
    "print(\"Cantidad de columnas: \",datos_tabla_parquet.num_columns)\n",
    "print(\"Nombre columnas: \",datos_tabla_parquet.column_names)\n",
    "print(\"Cantidad de registros: \",datos_tabla_parquet.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Datasets: user`\n",
    "\n",
    "Tipo de archivo Parquet\n",
    "\n",
    "Procedimieto que permite separar el dataset user en 10 archivos tipo parquet con aproximadamente 150.000 registros cada archivo.<br>\n",
    "El datasets user tiene aproximadamente: 2.105.597 registros.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente procedimiento lee el datasets original Parquet, aproximadamente de 2.6Gb y lo separa en archivos tipo parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_user_parquet = \"../Datasets/Datasets_Yelp/user/user.parquet\"             # Ruta y nombre del Archivo original\n",
    "datos_user_parquet = pq.read_table(archivo_user_parquet)                        # Se lee y se carga el archivo como una tabla\n",
    "cantidad_registros = 150000                                                     # Cantidad de registros por archivos separados\n",
    "for ind in range(0, datos_user_parquet.num_rows, cantidad_registros):           # Un ciclo para grupo de registros\n",
    "    info_cantidad_registros = datos_user_parquet.slice(ind, cantidad_registros) # Se extrae grupos de cantidad_registros registros\n",
    "    nombre_archivo = \"user_\" + str(ind // cantidad_registros) + \".parquet\"      # Se arma el nombre del archivo\n",
    "    ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/user/', nombre_archivo) # La ruta donde se guardara el archivo\n",
    "    pq.write_table(info_cantidad_registros, ruta_archivo)                       # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/user/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"user_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_user_concatenado = pd.concat(df_list)        # Se concatenan todos los DataFrames en uno solo\n",
    "df_user_concatenado = df_user_concatenado.reset_index(drop=True)    # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario de datos del dataset de user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Column | Dtype | RealType |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| 0 | user_id | object | string |\n",
    "| 1 | name| object | string | \n",
    "| 2 | review_count | int64 | int64 | \n",
    "| 3 | yelping_since | object | date YYYY-MM-DD |\n",
    "| 4 | useful | int64 | int64 | \n",
    "| 5 | funny | int64 | int64 |  \n",
    "| 6 | cool | int64 | int64 |\n",
    "| 7 | elite | object | list int64 |\n",
    "| 8 | friends | object | list string |\n",
    "| 9 | fans | int64 | int64 |\n",
    "| 10 | average_stars | float64 | float64 |\n",
    "| 11 | compliment_hot | int64 | int64 |  \n",
    "| 12 | compliment_more | int64 | int64 |  \n",
    "| 13 | compliment_profile | int64 | int64 |  \n",
    "| 14 | compliment_cute | int64 | int64 |  \n",
    "| 15 | compliment_list | int64 | int64 |  \n",
    "| 16 | compliment_note | int64 | int64 |  \n",
    "| 17 | compliment_plain | int64 | int64 |  \n",
    "| 18 | compliment_cool | int64 | int64 |  \n",
    "| 19 | compliment_funny | int64 | int64 |  \n",
    "| 20 | compliment_writer | int64 | int64 |  \n",
    "| 21 | compliment_photos | int64 | int64 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su manipulacion se reduce el dataset a 5 años de registros y se ajustan los campos:<br> \n",
    ". yelping_since al formato: YYYY-MM-DD<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales desde:  2004-10-12 08:46:11 hasta:  2022-01-19 17:15:47\n",
      "Datos finales desde:  2018-01-01 hasta:  2022-01-19\n"
     ]
    }
   ],
   "source": [
    "print('Datos iniciales desde: ', df_user_concatenado['yelping_since'].min(), 'hasta: ',df_user_concatenado['yelping_since'].max())\n",
    "df_user_concatenado['yelping_since'] = pd.to_datetime(df_user_concatenado['yelping_since'])\n",
    "df_user_2018_to_2022 = df_user_concatenado[df_user_concatenado['yelping_since'].dt.year.between(2018, 2022)]\n",
    "df_user_2018_to_2022 = df_user_2018_to_2022.reset_index(drop=True)\n",
    "df_user_2018_to_2022['yelping_since'] = pd.to_datetime(df_user_2018_to_2022['yelping_since']).dt.strftime('%Y-%m-%d')\n",
    "print('Datos finales desde: ', df_user_2018_to_2022['yelping_since'].min(), 'hasta: ',df_user_2018_to_2022['yelping_since'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un archivo en el directorio user con el dataset reducido en cantidad de registros para 5 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_user_2018_to_2022)                              # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "nombre_archivo = \"user-reducido.parquet\"                                        # Se arma el nombre del archivo\n",
    "ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/user/', nombre_archivo)  # La ruta donde se guardara el archivo\n",
    "pq.write_table(table, ruta_archivo)                                             # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a limitaciones del repositorio de GitHob, donde solo permite archivos de tamaño maximo de 100MB, se procede a generar archivos que no excedan dicho tamaño.<br>\n",
    ". Cantidad total de registros:  329174<br>\n",
    ". Cantidad de registros aproximado por archivos: 50000<br>\n",
    ". Cantidad de archivos: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedimiento para crear archivos separados de grupos de registros donde el tamaño de cada archivo no exceda de 50Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_user_parquet = \"../Datasets/Datasets_Yelp/user/user-reducido.parquet\"   # Ruta y nombre del Archivo original\n",
    "datos_user_parquet = pq.read_table(archivo_user_parquet)                        # Se lee y se carga el archivo como una tabla\n",
    "cantidad_registros = 50000                                                      # Cantidad de registros por archivos separados\n",
    "for ind in range(0, datos_user_parquet.num_rows, cantidad_registros):           # Un ciclo para grupo de registros\n",
    "    info_cantidad_registros = datos_user_parquet.slice(ind, cantidad_registros) # Se extrae grupos de cantidad_registros registros\n",
    "    nombre_archivo = \"user-reducido_\" + str(ind // cantidad_registros) + \".parquet\"  # Se arma el nombre del archivo\n",
    "    ruta_archivo = os.path.join('../Datasets/Datasets_Yelp/user/', nombre_archivo) # La ruta donde se guardara el archivo\n",
    "    pq.write_table(info_cantidad_registros, ruta_archivo)                       # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando los archivos parquet reducidos como un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/user/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"user-reducido_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_user_reducido_concatenado = pd.concat(df_list)                                   # Se concatenan todos los DataFrames en uno solo\n",
    "df_user_reducido_concatenado = df_user_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>friends</th>\n",
       "      <th>fans</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RDlBwvQe5oEQ8STtj6a5rg</td>\n",
       "      <td>Dee</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>FrLfQoTXb7k68ZqBQTHSew, 88wN6ysc8Z1LSgaa-Au2MA...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wZ5mQSbZR5UpGpnddGT7mw</td>\n",
       "      <td>Odalis</td>\n",
       "      <td>178</td>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>542</td>\n",
       "      <td>95</td>\n",
       "      <td>429</td>\n",
       "      <td>2018,2019,20,20,2021</td>\n",
       "      <td>zGAGITTaxhi2J14604F_iA, WoMHyzfYFI8juti3DbaIvg...</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oj8Ohm_2OlV6hjKMsQn69w</td>\n",
       "      <td>Emily</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>YQhTRxurCw9uNe3stFyHHA, XUbfn0nkZgcsLixJJ_2WKQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LRjvkbcZ5S4efH0xAVfkxw</td>\n",
       "      <td>Maggie</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-09-08</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>VNHx2MZANbDBZml5eNMP2g, 5CmrzlkGIglc5A-4b4S8Ig...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id    name  review_count yelping_since  useful  funny  \\\n",
       "0  RDlBwvQe5oEQ8STtj6a5rg     Dee             4    2018-11-06       5      0   \n",
       "1  wZ5mQSbZR5UpGpnddGT7mw  Odalis           178    2018-04-15     542     95   \n",
       "2  Oj8Ohm_2OlV6hjKMsQn69w   Emily             2    2018-07-01       1      0   \n",
       "3  LRjvkbcZ5S4efH0xAVfkxw  Maggie             1    2018-09-08       2      0   \n",
       "\n",
       "   cool                 elite  \\\n",
       "0     2                         \n",
       "1   429  2018,2019,20,20,2021   \n",
       "2     0                         \n",
       "3     0                         \n",
       "\n",
       "                                             friends  fans  ...  \\\n",
       "0  FrLfQoTXb7k68ZqBQTHSew, 88wN6ysc8Z1LSgaa-Au2MA...     0  ...   \n",
       "1  zGAGITTaxhi2J14604F_iA, WoMHyzfYFI8juti3DbaIvg...    22  ...   \n",
       "2  YQhTRxurCw9uNe3stFyHHA, XUbfn0nkZgcsLixJJ_2WKQ...     0  ...   \n",
       "3  VNHx2MZANbDBZml5eNMP2g, 5CmrzlkGIglc5A-4b4S8Ig...     0  ...   \n",
       "\n",
       "   compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n",
       "0                0                   0                0                0   \n",
       "1                3                   0                0                0   \n",
       "2                0                   0                0                0   \n",
       "3                0                   0                0                0   \n",
       "\n",
       "   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n",
       "0                0                 0                0                 0   \n",
       "1               21                29               33                33   \n",
       "2                0                 0                0                 0   \n",
       "3                0                 0                0                 0   \n",
       "\n",
       "   compliment_writer  compliment_photos  \n",
       "0                  0                  0  \n",
       "1                 21                 26  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_reducido_concatenado.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Datasets: business`\n",
    "\n",
    "Tipo de archivo csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el archivo .csv, se elimina la columna Unnamed y se resetea el index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (15,16,17,18,19,20,26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ruta_archivo_cardano = \"../Datasets/Datasets_Yelp/business/df_business_yelp.csv\"\n",
    "df_business_yelp = pd.read_csv(ruta_archivo_cardano)\n",
    "# Se elimina la columna Unnamed del dataframe\n",
    "df_business_yelp = df_business_yelp.loc[:, ~df_business_yelp.columns.str.contains('^Unnamed')]\n",
    "# Se eliminan las columnas repetidas que contienen .1 en su nombre\n",
    "df_business_yelp = df_business_yelp.loc[:, ~df_business_yelp.columns.str.contains('.1')]\n",
    "# Se eliminan los registros duplicadas\n",
    "df_business_yelp.drop_duplicates(inplace=True)\n",
    "# Se eliminan los registros que contienen null en el campo state\n",
    "df_business_yelp = df_business_yelp[df_business_yelp['state'].notna()]\n",
    "df_business_yelp = df_business_yelp[df_business_yelp['address'].notna()]\n",
    "# Se resetea el index\n",
    "df_business_yelp = df_business_yelp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar los registros que tienen Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id         0\n",
       "name                0\n",
       "address             0\n",
       "city                0\n",
       "state               0\n",
       "postal_code        17\n",
       "latitude            0\n",
       "longitude           0\n",
       "stars               0\n",
       "review_count        0\n",
       "is_open             0\n",
       "attributes      12873\n",
       "categories        102\n",
       "hours           22112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business_yelp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan los estados que no seran tomados en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_estados = ['Tennessee', 'Ohio', 'North Carolina', 'Georgia', 'Texas',\n",
    "                 'Michigan', 'Alabama', 'Indiana', 'Arizona', 'Florida']\n",
    "df_list = []                 \n",
    "for estado in lista_estados:                 \n",
    "    df = df_business_yelp[df_business_yelp.address.str.contains(estado)]   \n",
    "    df_list.append(df)  \n",
    "df_business_yelp_reducido = pd.concat(df_list)   \n",
    "df_business_yelp_reducido = df_business_yelp_reducido.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tennessee Esta en:  6  direcciones\n",
      "Ohio Esta en:  44  direcciones\n",
      "North Carolina Esta en:  0  direcciones\n",
      "Georgia Esta en:  20  direcciones\n",
      "Texas Esta en:  2  direcciones\n",
      "Michigan Esta en:  283  direcciones\n",
      "Alabama Esta en:  33  direcciones\n",
      "Indiana Esta en:  75  direcciones\n",
      "Arizona Esta en:  33  direcciones\n",
      "Florida Esta en:  379  direcciones\n"
     ]
    }
   ],
   "source": [
    "for estado in lista_estados: \n",
    "    print(estado, \"Esta en: \", df_business_yelp.address.str.contains(estado).sum(), \" direcciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tennessee Esta en:  6  direcciones\n",
      "Ohio Esta en:  44  direcciones\n",
      "North Carolina Esta en:  0  direcciones\n",
      "Georgia Esta en:  20  direcciones\n",
      "Texas Esta en:  2  direcciones\n",
      "Michigan Esta en:  283  direcciones\n",
      "Alabama Esta en:  33  direcciones\n",
      "Indiana Esta en:  75  direcciones\n",
      "Arizona Esta en:  33  direcciones\n",
      "Florida Esta en:  379  direcciones\n"
     ]
    }
   ],
   "source": [
    "for estado in lista_estados: \n",
    "    print(estado, \"Esta en: \", df_business_yelp_reducido.address.str.contains(estado).sum(), \" direcciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/user/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"user-reducido_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_user_reducido_concatenado = pd.concat(df_list)                                   # Se concatenan todos los DataFrames en uno solo\n",
    "df_user_reducido_concatenado = df_user_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145216, 14)\n",
      "(875, 14)\n"
     ]
    }
   ],
   "source": [
    "print(df_business_yelp.shape)\n",
    "print(df_business_yelp_reducido.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437       False\n",
      "452       False\n",
      "1124      False\n",
      "1359      False\n",
      "2346      False\n",
      "          ...  \n",
      "142831    False\n",
      "142886    False\n",
      "142911    False\n",
      "143297    False\n",
      "144058    False\n",
      "Name: address, Length: 379, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print((df_business_yelp_reducido['address'] == 'Ohio'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se salva el archivo en formato tipo parqet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business_yelp.to_parquet('../Datasets/Datasets_Yelp/business/business.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/business/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"business*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_business_concatenado = pd.concat(df_list)    # Se concatenan todos los DataFrames en uno solo\n",
    "df_business_concatenado = df_business_concatenado.reset_index(drop=True)    # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145216 entries, 0 to 145215\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   business_id   145216 non-null  object \n",
      " 1   name          145216 non-null  object \n",
      " 2   address       145216 non-null  object \n",
      " 3   city          145216 non-null  object \n",
      " 4   state         145216 non-null  object \n",
      " 5   postal_code   145199 non-null  object \n",
      " 6   latitude      145216 non-null  float64\n",
      " 7   longitude     145216 non-null  float64\n",
      " 8   stars         145216 non-null  float64\n",
      " 9   review_count  145216 non-null  int64  \n",
      " 10  is_open       145216 non-null  int64  \n",
      " 11  attributes    132343 non-null  object \n",
      " 12  categories    145114 non-null  object \n",
      " 13  hours         123104 non-null  object \n",
      "dtypes: float64(3), int64(2), object(9)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_business_concatenado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n",
       "      <td>mWMc6_wTdE0EUBKIGXDVfA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>St Honore Pastries</td>\n",
       "      <td>Perkiomen Valley Brewery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>935 Race St</td>\n",
       "      <td>101 Walnut St</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Green Lane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>CA</td>\n",
       "      <td>MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postal_code</th>\n",
       "      <td>19107</td>\n",
       "      <td>18054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>39.9555</td>\n",
       "      <td>40.3382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>-75.1556</td>\n",
       "      <td>-75.4717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_count</th>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_open</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attributes</th>\n",
       "      <td>{'RestaurantsDelivery': 'False', 'OutdoorSeati...</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True', 'Wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <td>Restaurants, Food, Bubble Tea, Coffee &amp; Tea, B...</td>\n",
       "      <td>Brewpubs, Breweries, Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours</th>\n",
       "      <td>{'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...</td>\n",
       "      <td>{'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              0  \\\n",
       "business_id                              MTSW4McQd7CbVtyjqoe9mw   \n",
       "name                                         St Honore Pastries   \n",
       "address                                             935 Race St   \n",
       "city                                               Philadelphia   \n",
       "state                                                        CA   \n",
       "postal_code                                               19107   \n",
       "latitude                                                39.9555   \n",
       "longitude                                              -75.1556   \n",
       "stars                                                         4   \n",
       "review_count                                                 80   \n",
       "is_open                                                       1   \n",
       "attributes    {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
       "categories    Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
       "hours         {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...   \n",
       "\n",
       "                                                              1  \n",
       "business_id                              mWMc6_wTdE0EUBKIGXDVfA  \n",
       "name                                   Perkiomen Valley Brewery  \n",
       "address                                           101 Walnut St  \n",
       "city                                                 Green Lane  \n",
       "state                                                        MO  \n",
       "postal_code                                               18054  \n",
       "latitude                                                40.3382  \n",
       "longitude                                              -75.4717  \n",
       "stars                                                       4.5  \n",
       "review_count                                                 13  \n",
       "is_open                                                       1  \n",
       "attributes    {'BusinessAcceptsCreditCards': 'True', 'Wheelc...  \n",
       "categories                            Brewpubs, Breweries, Food  \n",
       "hours         {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business_concatenado.head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "+ DataSet de Google Maps !!!\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Datasets: metadata-sitios`\n",
    "\n",
    "Tipos de archivos JSon\n",
    "\n",
    "El directorio metadata-sitios, esta compuesto por un grupo de archivos JSon, cada archivo contiene aproximadamente: 275.001 registros, por lo que se crea un procedimiento que permite crear de forma automatica un archivo parquet para cada uno de los archivos JSon en el directorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente procedimiento lee los datasets original JSon, aproximadamente de 870Mb y lo separa en archivos tipo parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datsets/Datasets_Google_Maps/metadata-sitios/\"                     # Ruta del directorio con los archivos JSon con la metadata-sitios\n",
    "archivos_metadatos_sitios = os.listdir(directorio)                                  # Se guarda el listado de los archivos contenidos en el directorio\n",
    "\n",
    "for archivo in archivos_metadatos_sitios:                                           # Se realiza un ciclo para cada archivo en el directorio\n",
    "    archivo_metadata_sitios_json = directorio + archivo                             # Se le asigna la ruta y el nombre del archivo.\n",
    "    with jsonlines.open(archivo_metadata_sitios_json) as reader:                    # Se obtiene el grupo de registros por cada archivo\n",
    "        lista_datos = list(reader)                                                  # Lee todos los registros y crea una lista\n",
    "        total_registros = len(lista_datos)                                          # Se tiene el total de registros, esto para separar en varios archivos.\n",
    "        for ind in range(0, total_registros, total_registros):                      # Se crea un archivo parquet para cada grupo de registors\n",
    "            iformacion_registros = lista_datos[ind:ind+total_registros]             # Se extrae grupos de cantidad_registros registros\n",
    "            df = pd.DataFrame(iformacion_registros)                                 # Se crea un dataframe con el grupo de registros extraidos\n",
    "            table = pa.Table.from_pandas(df)                                        # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "            nombre_archivo = \"metadata-sitios_\" + archivo[0: archivo.index('.')] + \".parquet\"                   # Se arma el nombre del archivo\n",
    "            ruta_archivo = os.path.join('../Datsets/Datasets_Google_Maps/metadata-sitios/', nombre_archivo)     # La ruta donde se guardara el archivo\n",
    "            pq.write_table(table, ruta_archivo)                                     # Se crea el archivo con formato parquet en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datasets que contienen grupos de registros ya separados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Google_Maps/metadata-sitios/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"metadata-sitios_*.parquet\")\n",
    "df_list = []                                                          # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                                            # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_sitios = pq.read_table(source=archivo)                      # Se crea una tabla con cada archivo\n",
    "    df = tabla_sitios.to_pandas()                                     # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                                                # Se adicionan a los dataframe\n",
    "df_sitios_concatenado = pd.concat(df_list)                            # Se concatenan todos los DataFrames en uno solo\n",
    "df_sitios_concatenado = df_sitios_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario de datos metada-sitios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # | Column | Dtype | RealType |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| 0 | user_id | object | string |\n",
    "| 0 | name | object | string |\n",
    "| 1 | address | object | string |\n",
    "| 2 | gmap_id | object | string |\n",
    "| 3 | description | object | string |\n",
    "| 4 | latitude | float64 | float64 |\n",
    "| 5 | longitude | float64 | float64 |\n",
    "| 6 | category | object | list string |\n",
    "| 7 | avg_rating | float64 | float64 |\n",
    "| 8 | num_of_reviews | int64 | float64 | \n",
    "| 9 | price | object | float64 $$|\n",
    "| 10 | hours | object | list time|\n",
    "| 11 | MISC | object | list string |\n",
    "| 12 | state | object | string |\n",
    "| 13 | relative_results | object | list | \n",
    "| 14 | url | object | string |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trabaja con una copia de los registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3025011, 15)\n"
     ]
    }
   ],
   "source": [
    "print(df_sitios_concatenado.shape)\n",
    "df_sitios_concatenado_copia = df_sitios_concatenado.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan los registros que no tengan valor en el campo: address, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin datos None en address:  (2944500, 15)\n"
     ]
    }
   ],
   "source": [
    "df_sitios_concatenado_copia = df_sitios_concatenado_copia[df_sitios_concatenado_copia['address'].notna()]\n",
    "print(\"Sin datos None en address: \",df_sitios_concatenado_copia.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>Porter Pharmacy</td>\n",
       "      <td>City Textile</td>\n",
       "      <td>San Soo Dang</td>\n",
       "      <td>Nova Fabrics</td>\n",
       "      <td>Nobel Textile Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>Porter Pharmacy, 129 N Second St, Cochran, GA ...</td>\n",
       "      <td>City Textile, 3001 E Pico Blvd, Los Angeles, C...</td>\n",
       "      <td>San Soo Dang, 761 S Vermont Ave, Los Angeles, ...</td>\n",
       "      <td>Nova Fabrics, 2200 E 11th St, Los Angeles, CA ...</td>\n",
       "      <td>Nobel Textile Co, 719 E 9th St, Los Angeles, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmap_id</th>\n",
       "      <td>0x88f16e41928ff687:0x883dad4fd048e8f8</td>\n",
       "      <td>0x80c2c98c0e3c16fd:0x29ec8a728764fdf9</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>0x80c2c89923b27a41:0x32041559418d447</td>\n",
       "      <td>0x80c2c632f933b073:0xc31785961fe826a6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>32.3883</td>\n",
       "      <td>34.0189</td>\n",
       "      <td>34.0581</td>\n",
       "      <td>34.0237</td>\n",
       "      <td>34.0367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>-83.3571</td>\n",
       "      <td>-118.215</td>\n",
       "      <td>-118.292</td>\n",
       "      <td>-118.233</td>\n",
       "      <td>-118.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>[Pharmacy]</td>\n",
       "      <td>[Textile exporter]</td>\n",
       "      <td>[Korean restaurant]</td>\n",
       "      <td>[Fabric store]</td>\n",
       "      <td>[Fabric store]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_rating</th>\n",
       "      <td>4.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_of_reviews</th>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours</th>\n",
       "      <td>[[Friday, 8AM–6PM], [Saturday, 8AM–12PM], [Sun...</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Thursday, 6:30AM–6PM], [Friday, 6:30AM–6PM],...</td>\n",
       "      <td>[[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...</td>\n",
       "      <td>[[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>{'Accessibility': ['Wheelchair accessible entr...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'Accessibility': ['Wheelchair accessible entr...</td>\n",
       "      <td>{'Accessibility': None, 'Activities': None, 'A...</td>\n",
       "      <td>{'Accessibility': None, 'Activities': None, 'A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>Open ⋅ Closes 6PM</td>\n",
       "      <td>Open now</td>\n",
       "      <td>Open ⋅ Closes 6PM</td>\n",
       "      <td>Open ⋅ Closes 5PM</td>\n",
       "      <td>Open ⋅ Closes 5PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_results</th>\n",
       "      <td>[0x88f16e41929435cf:0x5b2532a2885e9ef6, 0x88f1...</td>\n",
       "      <td>[0x80c2c624136ea88b:0xb0315367ed448771, 0x80c2...</td>\n",
       "      <td>[0x80c2c78249aba68f:0x35bf16ce61be751d, 0x80c2...</td>\n",
       "      <td>[0x80c2c8811477253f:0x23a8a492df1918f7, 0x80c2...</td>\n",
       "      <td>[0x80c2c62c496083d1:0xdefa11317fe870a1, 0x80c2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  0  \\\n",
       "name                                                Porter Pharmacy   \n",
       "address           Porter Pharmacy, 129 N Second St, Cochran, GA ...   \n",
       "gmap_id                       0x88f16e41928ff687:0x883dad4fd048e8f8   \n",
       "description                                                    None   \n",
       "latitude                                                    32.3883   \n",
       "longitude                                                  -83.3571   \n",
       "category                                                 [Pharmacy]   \n",
       "avg_rating                                                      4.9   \n",
       "num_of_reviews                                                   16   \n",
       "price                                                          None   \n",
       "hours             [[Friday, 8AM–6PM], [Saturday, 8AM–12PM], [Sun...   \n",
       "MISC              {'Accessibility': ['Wheelchair accessible entr...   \n",
       "state                                             Open ⋅ Closes 6PM   \n",
       "relative_results  [0x88f16e41929435cf:0x5b2532a2885e9ef6, 0x88f1...   \n",
       "url               https://www.google.com/maps/place//data=!4m2!3...   \n",
       "\n",
       "                                                                  1  \\\n",
       "name                                                   City Textile   \n",
       "address           City Textile, 3001 E Pico Blvd, Los Angeles, C...   \n",
       "gmap_id                       0x80c2c98c0e3c16fd:0x29ec8a728764fdf9   \n",
       "description                                                    None   \n",
       "latitude                                                    34.0189   \n",
       "longitude                                                  -118.215   \n",
       "category                                         [Textile exporter]   \n",
       "avg_rating                                                      4.5   \n",
       "num_of_reviews                                                    6   \n",
       "price                                                          None   \n",
       "hours                                                          None   \n",
       "MISC                                                           None   \n",
       "state                                                      Open now   \n",
       "relative_results  [0x80c2c624136ea88b:0xb0315367ed448771, 0x80c2...   \n",
       "url               https://www.google.com/maps/place//data=!4m2!3...   \n",
       "\n",
       "                                                                  2  \\\n",
       "name                                                   San Soo Dang   \n",
       "address           San Soo Dang, 761 S Vermont Ave, Los Angeles, ...   \n",
       "gmap_id                       0x80c2c778e3b73d33:0xbdc58662a4a97d49   \n",
       "description                                                    None   \n",
       "latitude                                                    34.0581   \n",
       "longitude                                                  -118.292   \n",
       "category                                        [Korean restaurant]   \n",
       "avg_rating                                                      4.4   \n",
       "num_of_reviews                                                   18   \n",
       "price                                                          None   \n",
       "hours             [[Thursday, 6:30AM–6PM], [Friday, 6:30AM–6PM],...   \n",
       "MISC              {'Accessibility': ['Wheelchair accessible entr...   \n",
       "state                                             Open ⋅ Closes 6PM   \n",
       "relative_results  [0x80c2c78249aba68f:0x35bf16ce61be751d, 0x80c2...   \n",
       "url               https://www.google.com/maps/place//data=!4m2!3...   \n",
       "\n",
       "                                                                  3  \\\n",
       "name                                                   Nova Fabrics   \n",
       "address           Nova Fabrics, 2200 E 11th St, Los Angeles, CA ...   \n",
       "gmap_id                        0x80c2c89923b27a41:0x32041559418d447   \n",
       "description                                                    None   \n",
       "latitude                                                    34.0237   \n",
       "longitude                                                  -118.233   \n",
       "category                                             [Fabric store]   \n",
       "avg_rating                                                      3.3   \n",
       "num_of_reviews                                                    6   \n",
       "price                                                          None   \n",
       "hours             [[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...   \n",
       "MISC              {'Accessibility': None, 'Activities': None, 'A...   \n",
       "state                                             Open ⋅ Closes 5PM   \n",
       "relative_results  [0x80c2c8811477253f:0x23a8a492df1918f7, 0x80c2...   \n",
       "url               https://www.google.com/maps/place//data=!4m2!3...   \n",
       "\n",
       "                                                                  4  \n",
       "name                                               Nobel Textile Co  \n",
       "address           Nobel Textile Co, 719 E 9th St, Los Angeles, C...  \n",
       "gmap_id                       0x80c2c632f933b073:0xc31785961fe826a6  \n",
       "description                                                    None  \n",
       "latitude                                                    34.0367  \n",
       "longitude                                                  -118.249  \n",
       "category                                             [Fabric store]  \n",
       "avg_rating                                                      4.3  \n",
       "num_of_reviews                                                    7  \n",
       "price                                                          None  \n",
       "hours             [[Thursday, 9AM–5PM], [Friday, 9AM–5PM], [Satu...  \n",
       "MISC              {'Accessibility': None, 'Activities': None, 'A...  \n",
       "state                                             Open ⋅ Closes 5PM  \n",
       "relative_results  [0x80c2c62c496083d1:0xdefa11317fe870a1, 0x80c2...  \n",
       "url               https://www.google.com/maps/place//data=!4m2!3...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sitios_concatenado_copia.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su manipulacion se reduce el dataset a solo los 10 estados a evaluar:<br>\n",
    "['Tennessee', 'review-Ohio', 'North_Carolina', 'Georgia', 'Texas', 'Michigan', 'Alabama', 'Indiana', 'Arizona', 'Florida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_estados = ['Tennessee', 'Ohio', 'North_Carolina', 'Georgia', 'Texas',\n",
    "                 'Michigan', 'Alabama', 'Indiana', 'Arizona', 'Florida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_estados = ['Arizona', 'Florida']\n",
    "lista_estados = ['Tennessee', 'Ohio', 'North Carolina', 'Georgia', 'Texas',\n",
    "                 'Michigan', 'Alabama', 'Indiana', 'Arizona', 'Florida']\n",
    "df_sitios_lista = [] \n",
    "for estado in lista_estados:\n",
    "    df_sitios = df_sitios_concatenado_copia[df_sitios_concatenado_copia.address.str.contains(estado)]\n",
    "    df_sitios_lista.append(df_sitios)\n",
    "df_sitios_reducidos = pd.concat(df_sitios_lista)\n",
    "df_sitios_reducidos = df_sitios_reducidos.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un archivo en el directorio metadata-sitios con el dataset reducido en cantidad de registros para 10 estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_sitios_reducidos)                                # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "nombre_archivo = \"metadata-sitios_reducido.parquet\"                              # Se arma el nombre del archivo\n",
    "ruta_archivo = os.path.join('../Datasets/Datasets_Google_Maps/metadata-sitios/', nombre_archivo)  # La ruta donde se guardara el archivo\n",
    "pq.write_table(table, ruta_archivo)                                              # Se graba en el directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los datos del metadata-sitios reducidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"../Datasets/Datasets_Google_Maps/metadata-sitios/metadata-sitios_reducido.parquet\"\n",
    "tabla_sitios = pq.read_table(source=archivo)                      # Se crea una tabla con cada archivo\n",
    "df_sitios = tabla_sitios.to_pandas()                              # Se transforma la tabla en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>category</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_of_reviews</th>\n",
       "      <th>price</th>\n",
       "      <th>hours</th>\n",
       "      <th>MISC</th>\n",
       "      <th>state</th>\n",
       "      <th>relative_results</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkey Creek</td>\n",
       "      <td>Turkey Creek, Tennessee</td>\n",
       "      <td>0x8864a58f0058d779:0x77f0f0f82ba4920e</td>\n",
       "      <td>None</td>\n",
       "      <td>35.998232</td>\n",
       "      <td>-87.389478</td>\n",
       "      <td>[River]</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.google.com/maps/place/Tennessee/da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shelter Insurance - Nathan Mizer</td>\n",
       "      <td>Shelter Insurance - Nathan Mizer, 107A W Tenne...</td>\n",
       "      <td>0x885c336f4bd36d55:0x480c0e91ddfd240d</td>\n",
       "      <td>None</td>\n",
       "      <td>36.026326</td>\n",
       "      <td>-84.243362</td>\n",
       "      <td>[Insurance agency, Auto insurance agency, Home...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'Accessibility': ['Wheelchair accessible entr...</td>\n",
       "      <td>None</td>\n",
       "      <td>[0x885c336fe87d6049:0x12141fa4a69f85f5, 0x885c...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flat Creek</td>\n",
       "      <td>Flat Creek, Tennessee 37878</td>\n",
       "      <td>0x885ea23c1631d3df:0xeb621b8b97f25df6</td>\n",
       "      <td>None</td>\n",
       "      <td>35.673797</td>\n",
       "      <td>-83.894229</td>\n",
       "      <td>[River]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.google.com/maps/place/Tennessee+37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hurricane Branch</td>\n",
       "      <td>Hurricane Branch, Tennessee 37882</td>\n",
       "      <td>0x885ea154ea071f21:0x1a1ec761c433c2a7</td>\n",
       "      <td>None</td>\n",
       "      <td>35.677006</td>\n",
       "      <td>-83.857697</td>\n",
       "      <td>[River]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.google.com/maps/place/Tennessee+37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autism Tennessee</td>\n",
       "      <td>Autism Tennessee, 2607 Winford Ave, Nashville,...</td>\n",
       "      <td>0x886467cd4482dcff:0x636916ab972b869d</td>\n",
       "      <td>None</td>\n",
       "      <td>36.114866</td>\n",
       "      <td>-86.753344</td>\n",
       "      <td>[Social services organization]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>[[Thursday, 10AM–3PM], [Friday, 10AM–3PM], [Sa...</td>\n",
       "      <td>{'Accessibility': ['Wheelchair accessible entr...</td>\n",
       "      <td>Closed ⋅ Opens 10AM</td>\n",
       "      <td>[0x8864657265da0d5f:0x8b83c74c7fb3b1a6, 0x8864...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "0                      Turkey Creek   \n",
       "1  Shelter Insurance - Nathan Mizer   \n",
       "2                        Flat Creek   \n",
       "3                  Hurricane Branch   \n",
       "4                  Autism Tennessee   \n",
       "\n",
       "                                             address  \\\n",
       "0                            Turkey Creek, Tennessee   \n",
       "1  Shelter Insurance - Nathan Mizer, 107A W Tenne...   \n",
       "2                        Flat Creek, Tennessee 37878   \n",
       "3                  Hurricane Branch, Tennessee 37882   \n",
       "4  Autism Tennessee, 2607 Winford Ave, Nashville,...   \n",
       "\n",
       "                                 gmap_id description   latitude  longitude  \\\n",
       "0  0x8864a58f0058d779:0x77f0f0f82ba4920e        None  35.998232 -87.389478   \n",
       "1  0x885c336f4bd36d55:0x480c0e91ddfd240d        None  36.026326 -84.243362   \n",
       "2  0x885ea23c1631d3df:0xeb621b8b97f25df6        None  35.673797 -83.894229   \n",
       "3  0x885ea154ea071f21:0x1a1ec761c433c2a7        None  35.677006 -83.857697   \n",
       "4  0x886467cd4482dcff:0x636916ab972b869d        None  36.114866 -86.753344   \n",
       "\n",
       "                                            category  avg_rating  \\\n",
       "0                                            [River]         3.8   \n",
       "1  [Insurance agency, Auto insurance agency, Home...         4.3   \n",
       "2                                            [River]         5.0   \n",
       "3                                            [River]         5.0   \n",
       "4                     [Social services organization]         5.0   \n",
       "\n",
       "   num_of_reviews price                                              hours  \\\n",
       "0               5  None                                               None   \n",
       "1               3  None                                               None   \n",
       "2               4  None                                               None   \n",
       "3               1  None                                               None   \n",
       "4               4  None  [[Thursday, 10AM–3PM], [Friday, 10AM–3PM], [Sa...   \n",
       "\n",
       "                                                MISC                state  \\\n",
       "0                                               None                 None   \n",
       "1  {'Accessibility': ['Wheelchair accessible entr...                 None   \n",
       "2                                               None                 None   \n",
       "3                                               None                 None   \n",
       "4  {'Accessibility': ['Wheelchair accessible entr...  Closed ⋅ Opens 10AM   \n",
       "\n",
       "                                    relative_results  \\\n",
       "0                                               None   \n",
       "1  [0x885c336fe87d6049:0x12141fa4a69f85f5, 0x885c...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  [0x8864657265da0d5f:0x8b83c74c7fb3b1a6, 0x8864...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.google.com/maps/place/Tennessee/da...  \n",
       "1  https://www.google.com/maps/place//data=!4m2!3...  \n",
       "2  https://www.google.com/maps/place/Tennessee+37...  \n",
       "3  https://www.google.com/maps/place/Tennessee+37...  \n",
       "4  https://www.google.com/maps/place//data=!4m2!3...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sitios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: metadata-estados`\n",
    "\n",
    "Compuesto por un directorio para cada estado y dentro un grupo de archivos tipo JSon\n",
    "\n",
    "El directorio metadata-estados, esta compuesto por un grupo de directoros, uno por cada estado, y un conjunto de archivos JSon, cada archivo contiene aproximadamente: 150.000 registros, por lo que se crea un procedimiento que permite crear de forma automatica un archivo parquet para cada uno de los archivos JSon en el directorio de cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_origen = \"../Datsets/Datasets_Google_Maps/reviews-estados/\"                  # Debido a la cantidad de directorios y archivos se permitira tener separados los archivos de origen.\n",
    "directorio_destino = \"../Datsets/Datasets_Google_Maps/reviews-estados/\"                 # Debido a la cantidad de directorios y archivos se permitira tener separados los archivos de destido.\n",
    "directorios_reviews_estados = os.listdir(directorio_origen)                             # Se obtiene un listado de todos los subdirectorios del directorio origen.\n",
    "for sub_dir in directorios_reviews_estados:                                             # Ciclo por cada subdirectorio\n",
    "    archivos_reviews_estados = os.listdir(directorio_origen + sub_dir)                  # Se guarda el directorio y el subdirectorio a utilizar\n",
    "    os.mkdir(directorio_destino + sub_dir)                                              # Directorio donde se colocaran los archivos parquet\n",
    "    for archivo in archivos_reviews_estados:                                            # Ciclo por cada archivo en cada subdirectorio\n",
    "        archivo_review_estados_json = directorio_origen + sub_dir + '/' + archivo       # Se guarda el directorio origen, el subdirectorio y el nombre del archivo para ser leido.\n",
    "        with jsonlines.open(archivo_review_estados_json) as reader:                     # Se lee el archivo JSon\n",
    "            lista_datos = list(reader)                                                  # Lee todos los registros y crea una lista\n",
    "            total_registros = len(lista_datos)                                          # Se tiene el total de registros, esto para separar en varios archivos.\n",
    "            for ind in range(0, total_registros, total_registros):                      # Un ciclo para los registros de cada archivo JSon\n",
    "                iformacion_registros = lista_datos[ind:ind+total_registros]             # Se extrae grupos de cantidad_registros registros\n",
    "                df = pd.DataFrame(iformacion_registros)                                 # Se crea un dataframe con el grupo de registros extraidos\n",
    "                table = pa.Table.from_pandas(df)                                        # Se crea una tabla tipo pyarrow del dataframe para guardarlo en archivos separados.\n",
    "                nombre_archivo = sub_dir + \"_\" + archivo[0: archivo.index('.')] + \".parquet\"   # Se arma el nombre del archivo\n",
    "                ruta_archivo = os.path.join(directorio_destino + sub_dir, nombre_archivo)      # La ruta donde se guardara el archivo\n",
    "                pq.write_table(table, ruta_archivo)                                     # Se crea el archivo parquet en el directorio destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando la informacion de solo un estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = 'Texas'\n",
    "directorio = \"../Datasets/Datasets_Google_Maps/reviews-estados/review-\" + nombre_archivo + \"/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"review-*.parquet\")\n",
    "df_list = []                                                            # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                                              # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_estados = pq.read_table(source=archivo)                       # Se crea una tabla con cada archivo\n",
    "    df = tabla_estados.to_pandas()                                      # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                                                  # Se adicionan a los dataframe\n",
    "df_estados_concatenado = pd.concat(df_list)                             # Se concatenan todos los DataFrames en uno solo\n",
    "df_estados_concatenado['estado'] = nombre_archivo\n",
    "df_estados_concatenado = df_estados_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo\n",
    "archivo_estado_csv = directorio + 'review-' + nombre_archivo + \".csv\"\n",
    "df_estados_concatenado.to_csv(archivo_estado_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250000, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivo_estado_csv = \"../Datasets/Datasets_Google_Maps/reviews-estados/review-North_Carolina/review-North_Carolina.csv\"\n",
    "df = pd.read_csv(archivo_estado_csv)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando la informacion de todos los estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_origen = \"../Datasets/Datasets_Google_Maps/reviews-estados/\"    # Debido a la cantidad de directorios y archivos se permitira tener separados los archivos de origen.\n",
    "directorios_reviews_estados = os.listdir(directorio_origen)                # Se obtiene un listado de todos los subdirectorios del directorio origen.\n",
    "for sub_dir in directorios_reviews_estados:                                # Ciclo por cada subdirectorio\n",
    "    nombre_archivo = sub_dir.replace(\"review-\", '')\n",
    "    archivos_reviews_estados = os.listdir(directorio_origen + sub_dir)     # Se guarda el directorio y el subdirectorio a utilizar\n",
    "    directorio = directorio_origen + sub_dir + \"/\"\n",
    "    ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"*.parquet\")\n",
    "    df_list = []                                                            # Un dataframe vacio para ir adicionando\n",
    "    for archivo in ls_archivo:                                              # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "        # print(directorio, nombre_archivo)\n",
    "        # print(ls_archivo)\n",
    "        tabla_estados = pq.read_table(source=archivo)                       # Se crea una tabla con cada archivo\n",
    "        df = tabla_estados.to_pandas()                                      # Se transforma la tabla en un dataframe\n",
    "        # df['estado'] = nombre_archivo\n",
    "        df_list.append(df)                                                  # Se adicionan a los dataframe\n",
    "    df_estados_concatenado = pd.concat(df_list)                             # Se concatenan todos los DataFrames en uno solo\n",
    "    df['estado'] = nombre_archivo\n",
    "df_estados_concatenado = df_estados_concatenado.reset_index(drop=True)      # Se resetea el indice a uno secuencial y consecutivo        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_origen = \"../Datasets/Datasets_Google_Maps/reviews-estados/\"    # Debido a la cantidad de directorios y archivos se permitira tener separados los archivos de origen.\n",
    "directorios_reviews_estados = os.listdir(directorio_origen)                # Se obtiene un listado de todos los subdirectorios del directorio origen.\n",
    "for sub_dir in directorios_reviews_estados:                                # Ciclo por cada subdirectorio\n",
    "    nombre_archivo = sub_dir.replace(\"review-\", '')\n",
    "    archivos_reviews_estados = os.listdir(directorio_origen + sub_dir)     # Se guarda el directorio y el subdirectorio a utilizar\n",
    "    directorio = directorio_origen + sub_dir + \"/\"\n",
    "    ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"*.csv\")\n",
    "    df_list = []                                                            # Un dataframe vacio para ir adicionando\n",
    "    for archivo in ls_archivo:                                              # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "        df = pd.read_csv(archivo)\n",
    "        df_list.append(df)                                                  # Se adicionan a los dataframe\n",
    "    df_estados_concatenado = pd.concat(df_list)                             # Se concatenan todos los DataFrames en uno solo\n",
    "df_estados_concatenado = df_estados_concatenado.reset_index(drop=True)      # Se resetea el indice a uno secuencial y consecutivo        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2850000, 9)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_estados_concatenado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2849980</th>\n",
       "      <th>2849981</th>\n",
       "      <th>2849982</th>\n",
       "      <th>2849983</th>\n",
       "      <th>2849984</th>\n",
       "      <th>2849985</th>\n",
       "      <th>2849986</th>\n",
       "      <th>2849987</th>\n",
       "      <th>2849988</th>\n",
       "      <th>2849989</th>\n",
       "      <th>2849990</th>\n",
       "      <th>2849991</th>\n",
       "      <th>2849992</th>\n",
       "      <th>2849993</th>\n",
       "      <th>2849994</th>\n",
       "      <th>2849995</th>\n",
       "      <th>2849996</th>\n",
       "      <th>2849997</th>\n",
       "      <th>2849998</th>\n",
       "      <th>2849999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>116027333961946450036</td>\n",
       "      <td>101682319947243887670</td>\n",
       "      <td>114076018014845789116</td>\n",
       "      <td>116410452070326430922</td>\n",
       "      <td>113346740550680886125</td>\n",
       "      <td>112172420842429498318</td>\n",
       "      <td>103423168954627189806</td>\n",
       "      <td>115332274602197044834</td>\n",
       "      <td>104445444863675602185</td>\n",
       "      <td>112970752410215005542</td>\n",
       "      <td>111541954676733314084</td>\n",
       "      <td>101455115901962250159</td>\n",
       "      <td>117050501385090268935</td>\n",
       "      <td>116209957654170876138</td>\n",
       "      <td>104864780221475782836</td>\n",
       "      <td>113432710667574630477</td>\n",
       "      <td>114437354870659097984</td>\n",
       "      <td>110393435648440523598</td>\n",
       "      <td>103517617368795862805</td>\n",
       "      <td>103232669323935534602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>Google Me</td>\n",
       "      <td>Chandrally Mahajan</td>\n",
       "      <td>Cynthia Hild</td>\n",
       "      <td>Chun Chun</td>\n",
       "      <td>Mary Whitley</td>\n",
       "      <td>raymond cribbs</td>\n",
       "      <td>Bonnie Martin</td>\n",
       "      <td>Leslie Nugent</td>\n",
       "      <td>Jackie Strickland</td>\n",
       "      <td>Greg Wainwright</td>\n",
       "      <td>FlipTheSwitch (FlipTheSwitch)</td>\n",
       "      <td>Susan Dugger</td>\n",
       "      <td>Steve Brooks</td>\n",
       "      <td>sherry malovich</td>\n",
       "      <td>Gary Peterson</td>\n",
       "      <td>Ricky Rice</td>\n",
       "      <td>John Davis</td>\n",
       "      <td>Georgia Mcfadden</td>\n",
       "      <td>Freddie Davis</td>\n",
       "      <td>Autumn Bozarth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1412308477217</td>\n",
       "      <td>1574728423657</td>\n",
       "      <td>1588809149105</td>\n",
       "      <td>1574728154852</td>\n",
       "      <td>1616522076274</td>\n",
       "      <td>1607454563101</td>\n",
       "      <td>1617391403010</td>\n",
       "      <td>1579124844264</td>\n",
       "      <td>1584470569359</td>\n",
       "      <td>1578174179270</td>\n",
       "      <td>1530637403425</td>\n",
       "      <td>1499618592061</td>\n",
       "      <td>1520484245437</td>\n",
       "      <td>1581576060516</td>\n",
       "      <td>1561087785711</td>\n",
       "      <td>1544911047205</td>\n",
       "      <td>1543609738956</td>\n",
       "      <td>1538099584340</td>\n",
       "      <td>1520106492564</td>\n",
       "      <td>1625153702009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They are miracle workers! I let my sister colo...</td>\n",
       "      <td>Tanning. 5-Stars! Extremely helpful and unders...</td>\n",
       "      <td>Hydro bed is the @#%$</td>\n",
       "      <td>Great place for hair 💇 nails 💅 and tanning! Gr...</td>\n",
       "      <td>LuAnn listens to what you want and then does i...</td>\n",
       "      <td>The stylist are very nice and did an amazing j...</td>\n",
       "      <td>Always great, good prices and offers nails enh...</td>\n",
       "      <td>Love Luanne she always does an awesome job on ...</td>\n",
       "      <td>Quick, friendly service. Very homey.</td>\n",
       "      <td>Nails look great.</td>\n",
       "      <td>Wish she stop having to move</td>\n",
       "      <td>Very professional.</td>\n",
       "      <td>Great service</td>\n",
       "      <td>Give great hair cuts and styling</td>\n",
       "      <td>Great place /  Good work</td>\n",
       "      <td>They definitely know what they are doing. My d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'url': array(['https://lh5.googleusercontent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp</th>\n",
       "      <td>{'text': 'Thank you for 5 Stars review.', 'tim...</td>\n",
       "      <td>{'text': 'Thanks for 5 Star Review. We are her...</td>\n",
       "      <td>{'text': \"We treat all of our patient's as Sta...</td>\n",
       "      <td>{'text': 'Sincere Thanks and really appreciate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'Thank you! My pleasure', 'time': 154...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'I am sorry if you feel you received ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmap_id</th>\n",
       "      <td>0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa</td>\n",
       "      <td>0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa</td>\n",
       "      <td>0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa</td>\n",
       "      <td>0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "      <td>0x88e61aa5df038465:0x580976b6e9c5313d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estado</th>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   2849980  \\\n",
       "user_id                              116027333961946450036   \n",
       "name                                             Google Me   \n",
       "time                                         1412308477217   \n",
       "rating                                                   5   \n",
       "text                                                   NaN   \n",
       "pics                                                   NaN   \n",
       "resp     {'text': 'Thank you for 5 Stars review.', 'tim...   \n",
       "gmap_id              0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849981  \\\n",
       "user_id                              101682319947243887670   \n",
       "name                                    Chandrally Mahajan   \n",
       "time                                         1574728423657   \n",
       "rating                                                   5   \n",
       "text                                                   NaN   \n",
       "pics                                                   NaN   \n",
       "resp     {'text': 'Thanks for 5 Star Review. We are her...   \n",
       "gmap_id              0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849982  \\\n",
       "user_id                              114076018014845789116   \n",
       "name                                          Cynthia Hild   \n",
       "time                                         1588809149105   \n",
       "rating                                                   5   \n",
       "text                                                   NaN   \n",
       "pics                                                   NaN   \n",
       "resp     {'text': \"We treat all of our patient's as Sta...   \n",
       "gmap_id              0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849983  \\\n",
       "user_id                              116410452070326430922   \n",
       "name                                             Chun Chun   \n",
       "time                                         1574728154852   \n",
       "rating                                                   5   \n",
       "text                                                   NaN   \n",
       "pics                                                   NaN   \n",
       "resp     {'text': 'Sincere Thanks and really appreciate...   \n",
       "gmap_id              0x88d9aaf5e68c3d11:0x6dded03ffc9a0caa   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849984  \\\n",
       "user_id                              113346740550680886125   \n",
       "name                                          Mary Whitley   \n",
       "time                                         1616522076274   \n",
       "rating                                                   5   \n",
       "text     They are miracle workers! I let my sister colo...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849985  \\\n",
       "user_id                              112172420842429498318   \n",
       "name                                        raymond cribbs   \n",
       "time                                         1607454563101   \n",
       "rating                                                   5   \n",
       "text     Tanning. 5-Stars! Extremely helpful and unders...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                       2849986  \\\n",
       "user_id                  103423168954627189806   \n",
       "name                             Bonnie Martin   \n",
       "time                             1617391403010   \n",
       "rating                                       5   \n",
       "text                     Hydro bed is the @#%$   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                                   2849987  \\\n",
       "user_id                              115332274602197044834   \n",
       "name                                         Leslie Nugent   \n",
       "time                                         1579124844264   \n",
       "rating                                                   5   \n",
       "text     Great place for hair 💇 nails 💅 and tanning! Gr...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849988  \\\n",
       "user_id                              104445444863675602185   \n",
       "name                                     Jackie Strickland   \n",
       "time                                         1584470569359   \n",
       "rating                                                   4   \n",
       "text     LuAnn listens to what you want and then does i...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849989  \\\n",
       "user_id                              112970752410215005542   \n",
       "name                                       Greg Wainwright   \n",
       "time                                         1578174179270   \n",
       "rating                                                   4   \n",
       "text     The stylist are very nice and did an amazing j...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849990  \\\n",
       "user_id                              111541954676733314084   \n",
       "name                         FlipTheSwitch (FlipTheSwitch)   \n",
       "time                                         1530637403425   \n",
       "rating                                                   5   \n",
       "text     Always great, good prices and offers nails enh...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                                   2849991  \\\n",
       "user_id                              101455115901962250159   \n",
       "name                                          Susan Dugger   \n",
       "time                                         1499618592061   \n",
       "rating                                                   5   \n",
       "text     Love Luanne she always does an awesome job on ...   \n",
       "pics                                                   NaN   \n",
       "resp                                                   NaN   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                       2849992  \\\n",
       "user_id                  117050501385090268935   \n",
       "name                              Steve Brooks   \n",
       "time                             1520484245437   \n",
       "rating                                       5   \n",
       "text      Quick, friendly service. Very homey.   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                       2849993  \\\n",
       "user_id                  116209957654170876138   \n",
       "name                           sherry malovich   \n",
       "time                             1581576060516   \n",
       "rating                                       4   \n",
       "text                         Nails look great.   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                       2849994  \\\n",
       "user_id                  104864780221475782836   \n",
       "name                             Gary Peterson   \n",
       "time                             1561087785711   \n",
       "rating                                       4   \n",
       "text              Wish she stop having to move   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                                   2849995  \\\n",
       "user_id                              113432710667574630477   \n",
       "name                                            Ricky Rice   \n",
       "time                                         1544911047205   \n",
       "rating                                                   4   \n",
       "text                                    Very professional.   \n",
       "pics                                                   NaN   \n",
       "resp     {'text': 'Thank you! My pleasure', 'time': 154...   \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                             Florida   \n",
       "\n",
       "                                       2849996  \\\n",
       "user_id                  114437354870659097984   \n",
       "name                                John Davis   \n",
       "time                             1543609738956   \n",
       "rating                                       5   \n",
       "text                             Great service   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                       2849997  \\\n",
       "user_id                  110393435648440523598   \n",
       "name                          Georgia Mcfadden   \n",
       "time                             1538099584340   \n",
       "rating                                       5   \n",
       "text          Give great hair cuts and styling   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                       2849998  \\\n",
       "user_id                  103517617368795862805   \n",
       "name                             Freddie Davis   \n",
       "time                             1520106492564   \n",
       "rating                                       5   \n",
       "text                  Great place /  Good work   \n",
       "pics                                       NaN   \n",
       "resp                                       NaN   \n",
       "gmap_id  0x88e61aa5df038465:0x580976b6e9c5313d   \n",
       "estado                                 Florida   \n",
       "\n",
       "                                                   2849999  \n",
       "user_id                              103232669323935534602  \n",
       "name                                        Autumn Bozarth  \n",
       "time                                         1625153702009  \n",
       "rating                                                   4  \n",
       "text     They definitely know what they are doing. My d...  \n",
       "pics     [{'url': array(['https://lh5.googleusercontent...  \n",
       "resp     {'text': 'I am sorry if you feel you received ...  \n",
       "gmap_id              0x88e61aa5df038465:0x580976b6e9c5313d  \n",
       "estado                                             Florida  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_estados_concatenado.tail(20).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
